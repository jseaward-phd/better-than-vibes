{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7a114a-2ce7-4fc4-a88c-b6a8f24a5d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenML Dataset\n",
      "==============\n",
      "Name..........: electricity\n",
      "Version.......: 13\n",
      "Format........: arff\n",
      "Upload Date...: 2022-07-10 10:34:54\n",
      "Licence.......: Public\n",
      "Download URL..: https://api.openml.org/data/v1/download/22103281/electricity.arff\n",
      "OpenML URL....: https://www.openml.org/d/44156\n",
      "# of features.: 9\n",
      "# of instances: 38474\n"
     ]
    }
   ],
   "source": [
    "# Import the module and get a dataset\n",
    "import btv\n",
    "\n",
    "# 44156 is the id for the \"electricity\" datasset, predicting whether the price of electricty rises or falls week to week.\n",
    "# Feel free to chose another from the openML catalogue (https://www.openml.org/search?type=data&status=active)\n",
    "# Make sure to update the name of the class column argument if necessary.\n",
    "df, X, y = btv.data_tab.dataset2df(44156, class_cols=[\"class\"], verbose=True)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# to reproduce results, uncomment the lines below and use these test/train sets instead of the random ones above\n",
    "import numpy as np\n",
    "train_idx = np.load('demo_support/electric_train_set.npy')\n",
    "test_idx = np.load('demo_support/electric_test_set.npy')\n",
    "X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ee45c-cd31-40c3-b648-e4cd3beb9286",
   "metadata": {},
   "source": [
    "### Label information without any data\n",
    "Without looking at the data or deploying any kind of model, the labels necessarily hold a lot of information. All we have to go on to guess the label of any given data point is the frequency of the label's occurrence. The information in each label, $y_i=c_i$, is written $$I(y_i) = -\\log_2[p(y_i=c_i)]$$ where the probability of label $y_i$ being the correct class for the sample, $c_i$, can only be crudely estimated. Note that data of the sample, $x_i$, plays no role and the base of the logarithm makes the unit of information the bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3974962-2955-459d-b369-386f3893ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the test set assuming p(label) == 1/# of classes:\n",
      " 7695.0\n",
      "Information in the test set assuming p(label) == 1/# of label ouccurances:\n",
      " 7694.651180995905\n"
     ]
    }
   ],
   "source": [
    "naive_test_info = btv.core.chance_info(y_test,use_freq=False)\n",
    "frequnecy_test_info = btv.core.chance_info(y_test,use_freq=True)\n",
    "print(\"Information in the test set assuming p(label) == 1/# of classes:\\n\", naive_test_info)\n",
    "print(\"Information in the test set assuming p(label) == 1/# of label ouccurances:\\n\", frequnecy_test_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb18cce-78ea-47c8-8cea-0fb50030a19a",
   "metadata": {},
   "source": [
    "The frequency-aware information should be *lower* than the one based only on the number of classes, that reflecting the actual distribution of labels better. If they are close, that means the samples are very evenly distributed between the classes.\n",
    "### Information remaining for a trained model\n",
    "After training, a model will be able to use the data to infer the label. Therefore the information in the label for a trained model will be much less than using just frequency statistics. It can be written $$I(y_i|x_i) = -\\log_2[p(y_i=c_i|x_i)],$$ where $p(y_i=c_i|x_i)$ is the probability estimation the classifier gives to the correct class. A *perfect* model would know every label with certainty from it's data, guess every label correctly with probability one, giving $I=0$ for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb940ff-44b9-4aac-bf76-c8986d825b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the test set from frqunecy stats: 7694.651\n",
      "Information remaining in test set for trained classifier: 1761.151\n",
      "Score when trained on full training set: : 0.81092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier # a classic for tabulat data\n",
    "# feel free to use another classifier. The methods called will be .fit(X,y), .predict_proba(X), and .score(X,y)\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0, max_depth=1, random_state=10\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "predicted_label_probabilities = clf.predict_proba(X_test)\n",
    "trained_test_info = btv.core.prediction_info(y_test, predicted_label_probabilities).sum() # function returns the information of each label\n",
    "full_train_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Information in the test set from frqunecy stats: {frequnecy_test_info:.3f}\")\n",
    "print(f\"Information remaining in test set for trained classifier: {trained_test_info:.3f}\")\n",
    "print(f\"Score when trained on full training set: : {full_train_score:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08dcab-2a13-4e48-9dbf-c0ae62d30d6d",
   "metadata": {},
   "source": [
    "Since the Shannon information of message can be thought of as the amount of information needed to specify the message, a better-trained model need less information about (or \"leave less information in\" if you use the thermodynamic analogy of information being hidden in a system) a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0377d2ea-d47b-462a-a85c-4d562c98ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the test set from frqunecy stats: 7694.651\n",
      "Information remaining in test set for fully-trained classifier: 1761.151\n",
      "Score when trained on full training set: : 0.81092\n",
      "Information remaining in test set for minimum-trained classifier: 2889.147\n",
      "Score when trained on minimum training set: : 0.61663\n"
     ]
    }
   ],
   "source": [
    "# Select a minimum training set, one that only includes one of every label\n",
    "min_idx, _ = btv.core.collect_min_set(y_train)\n",
    "X_min, y_min = X_train[min_idx] , y_train[min_idx]\n",
    "\n",
    "# Fitting to this minimum set will result in a functional, but poor classifier\n",
    "clf.fit(X_min, y_min)\n",
    "min_trained_predicted_test_probabilities = clf.predict_proba(X_test)\n",
    "min_trained_test_info = btv.core.prediction_info(y_test, min_trained_predicted_test_probabilities).sum()\n",
    "min_trained_score = clf.score(X_test,y_test)\n",
    "\n",
    "print(f\"Information in the test set from frqunecy stats: {frequnecy_test_info:.3f}\")\n",
    "print(f\"Information remaining in test set for fully-trained classifier: {trained_test_info:.3f}\")\n",
    "print(f\"Score when trained on full training set: : {full_train_score:0.5f}\")\n",
    "print(f\"Information remaining in test set for minimum-trained classifier: {min_trained_test_info:.3f}\")\n",
    "print(f\"Score when trained on minimum training set: : {min_trained_score:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ddcba-d7a0-4ff2-b59b-b6269610f7ce",
   "metadata": {},
   "source": [
    "The amount of information left in a test set is more than just anoter metric for classifier performance. The more ignorant a classifier is about a set, the more it can potentially learn from it.\n",
    "#### Looking at the information remaining in the training set after training on subsets of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "389ec679-d252-4061-b6f7-398c5f1c0a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the training set from frqunecy stats: 30778.913\n",
      "Information remaining in train set for minimum-trained classifier: 11439.607\n",
      "Information remaining in train set for tenth-trained classifier: 7171.968\n",
      "Information remaining in train set for fully-trained classifier: 6907.605\n"
     ]
    }
   ],
   "source": [
    "frequnecy_train_info = btv.core.chance_info(y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "full_trained_train_info = btv.core.prediction_info(y_train, clf.predict_proba(X_train)).sum()\n",
    "clf.fit(X_train[:len(y_train)//10], y_train[:len(y_train)//10])\n",
    "tenth_trained_train_info = btv.core.prediction_info(y_train, clf.predict_proba(X_train)).sum()\n",
    "clf.fit(X_min, y_min)\n",
    "min_trained_train_info = btv.core.prediction_info(y_train, clf.predict_proba(X_train)).sum()\n",
    "\n",
    "print(f\"Information in the training set from frqunecy stats: {frequnecy_train_info:.3f}\")\n",
    "print(f\"Information remaining in train set for minimum-trained classifier: {min_trained_train_info:.3f}\")\n",
    "print(f\"Information remaining in train set for tenth-trained classifier: {tenth_trained_train_info:.3f}\")\n",
    "print(f\"Information remaining in train set for fully-trained classifier: {full_trained_train_info:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780f184-9744-4cb3-b518-400ac4165e9f",
   "metadata": {},
   "source": [
    "This raises the natural question of how much of the information in the train set a classifier extracts, the \"extraction rate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a28951-7d7c-4f45-a8c7-1d89b19e8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of information remaining in train set for minimum-trained classifier: 1.00000\n",
      "Fraction of information remaining in train set for tenth-trained classifier: 0.77871\n",
      "Fraction of information remaining in train set for fully-trained classifier: 0.77557\n"
     ]
    }
   ],
   "source": [
    "full_trained_train_info_rate = btv.core.extraction_rate(clf,X_train,y_train)\n",
    "tenth_trained_train_info_rate = btv.core.extraction_rate(clf,X_train[:len(y_train)//10],y_train[:len(y_train)//10])\n",
    "min_trained_train_info_rate =  btv.core.extraction_rate(clf,X_min,y_min)\n",
    "\n",
    "print(f\"Fraction of information remaining in train set for minimum-trained classifier: {min_trained_train_info_rate:.5f}\")\n",
    "print(f\"Fraction of information remaining in train set for tenth-trained classifier: {tenth_trained_train_info_rate:.5f}\")\n",
    "print(f\"Fraction of information remaining in train set for fully-trained classifier: {full_trained_train_info_rate:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc6438-b1e2-4cce-b5f4-901818c17baf",
   "metadata": {},
   "source": [
    "This gives a rough idea of how well a model learns from a training set, which a combination of how complex the data is and how well-suited the model's architecture and parameters are to the training data. Such a nest of dependancies do not make it a good evaluation tool.\n",
    "\n",
    "However, we can ask how much information *about* the test set a training set has for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c6ec68a-a1fb-44d7-9916-22fffe6b2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the test set from frqunecy stats: 7694.651\n",
      "Information remaining in test set for fully-trained classifier: 1761.151\n",
      "Score when trained on full training set: : 0.81092\n",
      "Information remaining in test set for minimum-trained classifier: 2889.147\n",
      "Score when trained on minimum training set: : 0.61663\n",
      "Information remaining in test set for classifier trained on almost all 0 labels: 3561.000\n",
      "Score when trained on almost all 0 labels : 0.53723\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "idx_0s = np.where(y_train == 0)[0] \n",
    "idx_0splus1 = np.unique(np.append(min_idx, idx_0s)) # a training set of almost all zeros will not tell us much about the test set\n",
    "clf.fit(X_train[idx_0splus1],y_train[idx_0splus1])\n",
    "almost_all_zero_test_info = btv.core.prediction_info(y_test, clf.predict_proba(X_test)).sum()\n",
    "\n",
    "print(f\"Information in the test set from frqunecy stats: {frequnecy_test_info:.3f}\")\n",
    "print(f\"Information remaining in test set for fully-trained classifier: {trained_test_info:.3f}\")\n",
    "print(f\"Score when trained on full training set: : {full_train_score:0.5f}\")\n",
    "print(f\"Information remaining in test set for minimum-trained classifier: {min_trained_test_info:.3f}\")\n",
    "print(f\"Score when trained on minimum training set: : {min_trained_score:0.5f}\")\n",
    "print(f\"Information remaining in test set for classifier trained on almost all 0 labels: {almost_all_zero_test_info:.3f}\")\n",
    "print(f\"Score when trained on almost all 0 labels : {clf.score(X_test,y_test):0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b85c26-f802-4b9d-9767-5a5b7b55b09c",
   "metadata": {},
   "source": [
    "NB: When using a random forest classifier on the electricity data set (OpenML id = 44156), and the train/test split aved in ./demo_support/, the classifier trained on almost all zeros did *worse* than on trained on only two samples. It left more information in the test set ($\\approx1100$ bits) and scored $\\approx8$% worse ($\\approx53$% accuracy). This is not too surprizing since the training set with only one sample of each labels reflects the class balance of the test set.\n",
    "\n",
    "ASking how much information diffent training sets provide about a test set may be interesting, but in practical terms, this is just another evaluation metric. It requires us to perform the expensive part of model developmen, training, which we are trying to minimize through data pruning and prioritization.\n",
    "\n",
    "# Training Data Selection\n",
    "So Shnnon information is behaving as expected, giving a quanitative measure of how ignorant a classifier is about certain set of labels. If we are in the position of having to chose a training set however, information alone is not enough. Information a set contains for a given classifier scales with its size, while performance (and information remaining in a test set) once trained does not. What is more, ideally, we would select training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da778ff-9b35-4054-9a9a-26085ebf7c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8424f1-60a6-4376-aeed-0d656f78401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing between fine-tuning sets\n",
    "ds = data_tab.getdata(44156, verbose=False)\n",
    "df, X, y = data_tab.dataset2df(ds, class_cols=[\"class\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0, max_depth=1, random_state=10\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_dev1, y_train, y_dev1 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev2, y_train, y_dev2 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev3, y_train, y_dev3 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev4, y_train, y_dev4 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev5, y_train, y_dev5 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "X_ft_list = [X_dev1, X_dev2, X_dev3, X_dev4, X_dev5]\n",
    "y_ft_list = [y_dev1, y_dev2, y_dev3, y_dev4, y_dev5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
