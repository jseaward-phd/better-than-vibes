{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5901208c-71e3-44e1-9886-32e628cc1927",
   "metadata": {},
   "source": [
    "# Better Than Vibes\n",
    "#### By J. Seaward\n",
    "\n",
    "While training and fine-tuning models models in a shoestring budget, the question of whether or not we had enough data was a constant problem. Is the problem with the model architecture? the implementation? or do we need more/better data? When I asked how we knew such and such data was enough the answer typically boiled down to \"vibes.\" Let us see if we can do ***better than vibes.***\n",
    "\n",
    "The goal at the outset was to be able to evaluate a fine-tuning or training set without (re)training the model.\n",
    "Have not come up with a way to check the sufficiency of a fine-tunin set in the absence of a task *and* model, nor uncovered a unversal scaling law for model parameters and sufficient dataset size. What we have come up with at this point is a way of prioritizing training data at both the sample and set level allowing for automatic pruning of training sets, allowing for cheaper/fast training without loss of performance, as well as ranking of potential fin-tuning sets. These methods rely on the information content of data labels and some simple geometric considerations. To make evaluation of the data model-independant, we construct a self-ignoring distance weighted k-nearest neighbor classifier (explained below), which is applicable well to any dataspace with a meaningful distance metric, or model whose outputs vary smoothly and slowly in data-space.\n",
    "\n",
    "Marginalia provided in the code blocks.\n",
    "\n",
    "## Introduction: Information Theory\n",
    "\n",
    "Shannon information, $I$, is the measure of information a character in a message brings to a receiver. The information a character $a$ brings is written $$I(a) = -\\log(p_a),$$ where $p_a$ is the probability that the character $a$ will be received. This probability is conditioned on the previous characters in the message *and* prior knowledge about the type of message. For example, a \"u\" following a \"q\" when the message is in English carries almost no information, since \"u\" nearly always follows \"q\" (conversely, any other letter carries large amounts of information, narrowing down the rest of the possible characters to only a handful). On the other hand, if the message were encrypted in such a way that all characters were equally likely, \"u\" after \"q\" would carry just as much information as any other character, them all having equal probability.\n",
    "\n",
    "Turning back to machine learning, we can think of each sample in a test set as a message, beginning with the data and ending with the label(s). A model, then, is a machine that guesses the label(s), conditioned on the data *and* what it has seen during training. What we want a model to do is learn the patterns that allow one to infer the next part of a message, given the first part. A language model trained on English text would have learned that \"u\" nearly allows follows \"q\", for instance. We can then think of each *label* in a data set as holding a certain amount of information, depending on what the model has already learned. From here, we can start asking questions about how much data is enough and which data sets will be more useful for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7a114a-2ce7-4fc4-a88c-b6a8f24a5d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenML Dataset\n",
      "==============\n",
      "Name..........: electricity\n",
      "Version.......: 13\n",
      "Format........: arff\n",
      "Upload Date...: 2022-07-10 10:34:54\n",
      "Licence.......: Public\n",
      "Download URL..: https://api.openml.org/data/v1/download/22103281/electricity.arff\n",
      "OpenML URL....: https://www.openml.org/d/44156\n",
      "# of features.: 9\n",
      "# of instances: 38474\n"
     ]
    }
   ],
   "source": [
    "# Import the module and get a dataset\n",
    "import btv\n",
    "\n",
    "# 44156 is the id for the \"electricity\" datasset, predicting whether the price of electricty rises or falls week to week.\n",
    "# Feel free to chose another from the openML catalogue (https://www.openml.org/search?type=data&status=active)\n",
    "# Make sure to update the name of the class column argument if necessary.\n",
    "df, X, y = btv.data_tabular.dataset2df(44156, class_cols=[\"class\"], verbose=True)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# to reproduce results, uncomment the lines below and use these test/train sets instead of the random ones above\n",
    "import numpy as np\n",
    "\n",
    "train_idx = np.load(\"demo_support/electric_train_set.npy\")\n",
    "test_idx = np.load(\"demo_support/electric_test_set.npy\")\n",
    "X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ee45c-cd31-40c3-b648-e4cd3beb9286",
   "metadata": {},
   "source": [
    "### Label information without any data\n",
    "Without looking at the data or deploying any kind of model, the labels necessarily hold a lot of information. All we have to go on to guess the label of any given data point is the frequency of the label's occurrence. The information in each label, $y_i=c_i$, is written $$I(y_i) = -\\log_2[p(y_i=c_i)]$$ where the probability of label $y_i$ being the correct class for the sample, $c_i$, can only be crudely estimated. Note that data of the sample, $x_i$, plays no role and the base of the logarithm makes the unit of information the bit. Call this information given only chance $I^*$, and consider the a prioiri information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3974962-2955-459d-b369-386f3893ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the test set assuming p(label) == 1/# of classes:\n",
      " 7695.0\n",
      "Information in the test set assuming p(label) == 1/# of label ouccurances:\n",
      " 7694.651180995905\n"
     ]
    }
   ],
   "source": [
    "naive_test_info = btv.core.chance_info(y_test, use_freq=False)\n",
    "frequnecy_test_info = btv.core.chance_info(y_test, use_freq=True)\n",
    "print(\n",
    "    \"Information in the test set assuming p(label) == 1/# of classes:\\n\",\n",
    "    naive_test_info,\n",
    ")\n",
    "print(\n",
    "    \"Information in the test set assuming p(label) == 1/# of label ouccurances:\\n\",\n",
    "    frequnecy_test_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb18cce-78ea-47c8-8cea-0fb50030a19a",
   "metadata": {},
   "source": [
    "The frequency-aware information should be *lower* than the one based only on the number of classes, that reflecting the actual distribution of labels better. If they are close, that means the samples are very evenly distributed between the classes.\n",
    "### Information remaining for a trained model\n",
    "After training, a model will be able to use the data to infer the label. Therefore the information in the label for a trained model will be much less than using just frequency statistics. It can be written $$I(y_i|x_i) = -\\log_2[p(y_i=c_i|x_i)],$$ where $p(y_i=c_i|x_i)$ is the probability estimation the classifier gives to the correct class. A *perfect* model would know every label with certainty from it's data, guess every label correctly with probability one, giving $I=0$ for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb940ff-44b9-4aac-bf76-c8986d825b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the test set from frqunecy stats: 7694.651\n",
      "Information remaining in test set for trained classifier: 1761.151\n",
      "Score when trained on full training set: : 0.81092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  # a classic for tabulat data\n",
    "\n",
    "# feel free to use another classifier. The methods called will be .fit(X,y), .predict_proba(X), and .score(X,y)\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0, max_depth=1, random_state=10\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "predicted_label_probabilities = clf.predict_proba(X_test)\n",
    "trained_test_info = btv.core.prediction_info(\n",
    "    y_test, predicted_label_probabilities\n",
    ").sum()  # function returns the information of each label\n",
    "full_train_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Information in the test set from frqunecy stats: {frequnecy_test_info:.3f}\")\n",
    "print(\n",
    "    f\"Information remaining in test set for trained classifier: {trained_test_info:.3f}\"\n",
    ")\n",
    "print(f\"Score when trained on full training set: : {full_train_score:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08dcab-2a13-4e48-9dbf-c0ae62d30d6d",
   "metadata": {},
   "source": [
    "Since the Shannon information of message can be thought of as the amount of information needed to specify the message, a better-trained model need less information about (or \"leave less information in\" if you use the thermodynamic analogy of information being hidden in a system) a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0377d2ea-d47b-462a-a85c-4d562c98ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the test set from frqunecy stats: 7694.651\n",
      "Information remaining in test set for fully-trained classifier: 1761.151\n",
      "Score when trained on full training set: : 0.81092\n",
      "Information remaining in test set for minimum-trained classifier: 2889.147\n",
      "Score when trained on minimum training set: : 0.61663\n",
      "Information remaining in test set for classifier trained on almost all 0 labels: 3561.000\n",
      "Score when trained on almost all 0 labels : 0.53723\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Select a minimum training set, one that only includes one of every label\n",
    "min_idx, _ = btv.core.collect_min_set(y_train)\n",
    "X_min, y_min = X_train[min_idx], y_train[min_idx]\n",
    "# Fitting to this minimum set will result in a functional, but poor classifier\n",
    "clf.fit(X_min, y_min)\n",
    "min_trained_predicted_test_probabilities = clf.predict_proba(X_test)\n",
    "min_trained_test_info = btv.core.prediction_info(\n",
    "    y_test, min_trained_predicted_test_probabilities\n",
    ").sum()\n",
    "min_trained_score = clf.score(X_test, y_test)\n",
    "\n",
    " # a training set of almost all zeros will not tell us much about the test set\n",
    "idx_0s = np.where(y_train == 0)[0]\n",
    "idx_0splus1 = np.unique(\n",
    "    np.append(min_idx, idx_0s)\n",
    ") \n",
    "clf.fit(X_train[idx_0splus1], y_train[idx_0splus1])\n",
    "almost_all_zero_test_info = btv.core.prediction_info(\n",
    "    y_test, clf.predict_proba(X_test)\n",
    ").sum()\n",
    "zeros_test_score = clf.score(X_test,y_test)\n",
    "\n",
    "print(f\"Information in the test set from frqunecy stats: {frequnecy_test_info:.3f}\")\n",
    "print(\n",
    "    f\"Information remaining in test set for fully-trained classifier: {trained_test_info:.3f}\"\n",
    ")\n",
    "print(f\"Score when trained on full training set: : {full_train_score:0.5f}\")\n",
    "print(\n",
    "    f\"Information remaining in test set for minimum-trained classifier: {min_trained_test_info:.3f}\"\n",
    ")\n",
    "print(f\"Score when trained on minimum training set: : {min_trained_score:0.5f}\")\n",
    "print(\n",
    "    f\"Information remaining in test set for classifier trained on almost all 0 labels: {almost_all_zero_test_info:.3f}\"\n",
    ")\n",
    "print(f\"Score when trained on almost all 0 labels : {zeros_test_score:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ddcba-d7a0-4ff2-b59b-b6269610f7ce",
   "metadata": {},
   "source": [
    "NB: When using a random forest classifier on the electricity data set (OpenML id = 44156), and the train/test split aved in ./demo_support/, the classifier trained on almost all zeros did *worse* than on trained on only two samples. It left more information in the test set ($\\approx1100$ bits) and scored $\\approx8$% worse ($\\approx53$% accuracy). This is not too surprizing since the training set with only one sample of each labels reflects the class balance of the test set.\n",
    "\n",
    "This gives us a way to quantify how much information a training set has for a classifier **about** a test set, but only if we are willing to retrain. In that sense it is only another evaluation metric and does not cut out the slow/expensive part of model development. However, when applied to training sets, the more ignorant a classifier is about a set, the more it can potentially learn from it.\n",
    "\n",
    "#### Looking at the information remaining in the training set after training on subsets of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389ec679-d252-4061-b6f7-398c5f1c0a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the training set from frqunecy stats: 30778.913\n",
      "Information remaining in train set for minimum-trained classifier: 11439.607\n",
      "Information remaining in train set for tenth-trained classifier: 7171.968\n",
      "Information remaining in train set for fully-trained classifier: 6907.605\n"
     ]
    }
   ],
   "source": [
    "frequnecy_train_info = btv.core.chance_info(y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "full_trained_train_info = btv.core.prediction_info(\n",
    "    y_train, clf.predict_proba(X_train)\n",
    ").sum()\n",
    "clf.fit(X_train[: len(y_train) // 10], y_train[: len(y_train) // 10])\n",
    "tenth_trained_train_info = btv.core.prediction_info(\n",
    "    y_train, clf.predict_proba(X_train)\n",
    ").sum()\n",
    "clf.fit(X_min, y_min)\n",
    "min_trained_train_info = btv.core.prediction_info(\n",
    "    y_train, clf.predict_proba(X_train)\n",
    ").sum()\n",
    "\n",
    "print(\n",
    "    f\"Information in the training set from frqunecy stats: {frequnecy_train_info:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Information remaining in train set for minimum-trained classifier: {min_trained_train_info:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Information remaining in train set for tenth-trained classifier: {tenth_trained_train_info:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Information remaining in train set for fully-trained classifier: {full_trained_train_info:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7e4ce-a439-478c-86c2-71d434ef07d9",
   "metadata": {},
   "source": [
    "The amount of informtion remaining in the entire training set may not be significantly lower when training on all of it vs. training on the whole thing (depending on the clssifier). This hints at the redundancy of training data. In order to find that redundancy, we introduce a modified distance weighted k-nearest neighbor model: the self-drop dknn.\n",
    "\n",
    "## Estimating Data Redudancy: The Self-Drop d-KNN\n",
    "A distance weight k-nearest neighbor classifier will take take the weighted average of the classes of the test point's $k$ nearest neighbors to dertimine the probability if its class. The average being weighted by $1/d_i$ where $d_i$ where $d_i$ is the distance to the $i^{th}$ neighbor. This assumes that data points whose features take simillar values (i.e. data points that are *close* in the data-space) are likely to be in the same class. Since this is such a broadly applicable assumption (e.g. metric embedding spaces, data ameanable to clustering, etc.) we take is as a basis for estimating the information a training set has about itself.\n",
    "\n",
    "A d-knn classifier assigns probabilites to all classes for each point in data space, whether labelled or not. The label on a labelled point provides the classifier with $I(y|x)=0$ because the classifier knows the label there with probability $1$. Likewise, the label on a point near one of the same class (see point highlighted in blue bellow), will provide very little information, since the probability assigned to the correct class is high. Conversely, a point near points of *unlike* class (see point highlighted in red) will provide much more information. Since the probabilities fall off with distance, points far from any others (e.g. the black \"x\" below) will still provide information, due to the lack of certainty there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64475d80-2059-491c-beb1-f3831d95eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPa0lEQVR4nO3deVxU5f4H8M+wDbLMICqbAoqKigsQbuAttFRck8pEfyZoalnaTbFSyyW1wq7XNQ01U1zTzK28pimKlmJuYKhpLgguoLkwxI7D8/tjYnIElNEZZuB83q/XeV3nOc+Z+c5p7syHc57zHJkQQoCIiIhIgixMXQARERGRqTAIERERkWQxCBEREZFkMQgRERGRZDEIERERkWQxCBEREZFkMQgRERGRZDEIERERkWQxCBEREZFkMQgRUZVJSEiATCZDQkKCqUshIgLAIERkNuLi4iCTySpcjhw5YuoSzc6VK1d09pG1tTXq1q2LkJAQfPjhh0hPT3/i575x4wY+/vhjJCcnG65gIjI7VqYugIh0zZgxA40aNSrT3qRJExNUUz0MGjQIvXr1QklJCe7du4djx45h/vz5WLBgAb7++msMHDhQ7+e8ceMGpk+fjoYNGyIgIMDwRRORWWAQIjIzPXv2RNu2bU1dRrXyzDPP4LXXXtNpS0tLQ/fu3REVFYUWLVrA39/fRNURkTnjqTGiambatGmwsLBAfHy8Tvsbb7wBGxsbnDp1CgBQVFSEqVOnIigoCEqlEvb29nj22Wexf/9+ne1KTy/997//xeLFi+Hj4wM7Ozt0794dV69ehRACM2fORIMGDVCrVi3069cPd+/e1XmOhg0bok+fPvjpp58QEBAAW1tb+Pn5YcuWLZV6T7/++it69OgBpVIJOzs7hIaG4tChQ0+xlwBvb2/ExcWhqKgI//nPf7Ttd+/exXvvvYfWrVvDwcEBCoUCPXv21O43QDOWqV27dgCAYcOGaU+9xcXFAQB+/vlnvPrqq/Dy8oJcLoenpyfGjRuH/Px8nRoyMzMxbNgwNGjQAHK5HO7u7ujXrx+uXLmi7VPZfVeZuksVFBTg448/hq+vL2xtbeHu7o6XX34Zly5d0vYpKSnB/Pnz0bJlS9ja2sLV1RVvvvkm7t2798T7nKhaEkRkFlauXCkAiL1794o///xTZ7l9+7a2X1FRkQgMDBTe3t4iOztbCCHErl27BAAxc+ZMbb8///xTuLu7i+joaBEbGyv+85//iGbNmglra2uRlJSk7ZeamioAiICAAOHn5yfmzp0rJk+eLGxsbETHjh3Fhx9+KEJCQsTChQvFv//9byGTycSwYcN0avf29ha+vr7CyclJTJw4UcydO1e0bt1aWFhYiJ9++knbb//+/QKA2L9/v7YtPj5e2NjYiODgYDFnzhwxb9480aZNG2FjYyN+/fXXR+6z0tpnz55dYZ/GjRuLevXqaR8fO3ZMNG7cWEycOFEsXbpUzJgxQ9SvX18olUpx/fp1IYQQmZmZYsaMGQKAeOONN8SaNWvEmjVrxKVLl4QQQrzzzjuiV69e4rPPPhNLly4Vw4cPF5aWlqJ///46rx0SEiKUSqWYPHmyWL58ufjss89Ely5dxIEDB/Ted5WpWwgh7t+/L1544QUBQAwcOFAsWrRIxMTEiOeff15s27ZN22/EiBHCyspKjBw5UixZskRMmDBB2Nvbi3bt2omioqJH7neimoRBiMhMlAah8ha5XK7TNyUlRdjY2IgRI0aIe/fuifr164u2bduK4uJibZ/79++LwsJCne3u3bsnXF1dxeuvv65tKw0T9erVE1lZWdr2SZMmCQDC399f53kHDRokbGxsREFBgbbN29tbABCbN2/WtqlUKuHu7i4CAwO1bQ8HoZKSEtG0aVMRFhYmSkpKtP3y8vJEo0aNRLdu3R65zyoThPr16ycACJVKJYQQoqCgQKjV6jLPI5fLxYwZM7Rtx44dEwDEypUryzxnXl5embaYmBghk8lEWlqaEEKzrx9XmxCV33eVrXvFihUCgJg7d26Z1yrdxz///LMAINatW6ezvjRQP9xOVJPx1BiRmVm8eDH27Nmjs/z44486fVq1aoXp06dj+fLlCAsLw+3bt7Fq1SpYWf0z7M/S0hI2NjYANKdB7t69i/v376Nt27Y4efJkmdd99dVXoVQqtY87dOgAAHjttdd0nrdDhw4oKirC9evXdbb38PDASy+9pH2sUCgQGRmJpKQkZGZmlvtek5OTceHCBfzf//0f7ty5g9u3b+P27dvIzc3FCy+8gIMHD6KkpKSyu65cDg4OAIC//voLACCXy2FhofnqU6vVuHPnDhwcHNCsWbNy90t5atWqpf13bm4ubt++jZCQEAghkJSUpO1jY2ODhISEx55uqsy+q2zdmzdvRt26dfHOO++UeR2ZTAYA2LRpE5RKJbp166bd57dv30ZQUBAcHBzKnD4lqsk4WJrIzLRv375Sg6Xff/99bNiwAUePHsVnn30GPz+/Mn1WrVqFOXPm4Ny5cyguLta2l3dVmpeXl87j0lDk6elZbvvDP+5NmjTR/tCW8vX1BaAZh+Tm5lbmNS9cuAAAiIqKKv9NAlCpVKhdu3aF6x8nJycHAODo6AhAEwoXLFiAL7/8EqmpqVCr1dq+derUqdRzpqenY+rUqfj+++/L7AeVSgVAE1w+//xzjB8/Hq6urujYsSP69OmDyMjIMvuiMvuusnVfunQJzZo10wmvD7tw4QJUKhVcXFzKXX/r1q1K7AWimoFBiKiaunz5sjZIpKSklFm/du1aDB06FOHh4Xj//ffh4uICS0tLxMTE6AyaLWVpaVnu61TULoR4iuo1So/2zJ49u8JL1EuP6Dyp06dPw8XFBQqFAgDw2WefYcqUKXj99dcxc+ZMODs7w8LCAmPHjq3U0Se1Wo1u3brh7t27mDBhApo3bw57e3tcv34dQ4cO1XmOsWPHom/fvti2bRt2796NKVOmICYmBvv27UNgYKBe7+Np635QSUkJXFxcsG7dunLX16tXT6/nI6rOGISIqqGSkhIMHToUCoUCY8eOxWeffYb+/fvj5Zdf1vb57rvv4OPjgy1btugcbZg2bZpRarp48SKEEDqv9ccffwDQXBlVnsaNGwPQnArq2rWrwWtKTEzEpUuXdC6t/+6779ClSxd8/fXXOn2zsrJQt25d7eOHj9CUSklJwR9//IFVq1YhMjJS275nz55y+zdu3Bjjx4/H+PHjceHCBQQEBGDOnDlYu3attk9l9l1l627cuDF+/fVXFBcXw9rausKa9u7di06dOumc5iOSIo4RIqqG5s6di8OHD2PZsmWYOXMmQkJC8NZbb+H27dvaPqVHch48cvPrr78iMTHRKDXduHEDW7du1T7Ozs7G6tWrERAQUO5pMQAICgpC48aN8d///ld7CutBf/755xPXk5aWhqFDh8LGxgbvv/++tt3S0rLM0axNmzaVGfNkb28PQBM0HlTefhVCYMGCBTr98vLyUFBQoNPWuHFjODo6orCwUKe9MvuusnW/8soruH37NhYtWoSHlW4/YMAAqNVqzJw5s0yf+/fvl3nPRDUZjwgRmZkff/wR586dK9MeEhICHx8f/P7775gyZQqGDh2Kvn37AtDcniMgIABvv/02vv32WwBAnz59sGXLFrz00kvo3bs3UlNTsWTJEvj5+ZUbOp6Wr68vhg8fjmPHjsHV1RUrVqzAzZs3sXLlygq3sbCwwPLly9GzZ0+0bNkSw4YNQ/369XH9+nXs378fCoUCP/zww2Nf++TJk1i7di1KSkqQlZWFY8eOYfPmzZDJZFizZg3atGmj7dunTx/MmDEDw4YNQ0hICFJSUrBu3Tr4+PjoPGfjxo3h5OSEJUuWwNHREfb29ujQoQOaN2+Oxo0b47333sP169ehUCiwefPmMmOF/vjjD7zwwgsYMGAA/Pz8YGVlha1bt+LmzZtlZrquzL6rbN2RkZFYvXo1oqOjcfToUTz77LPIzc3F3r178fbbb6Nfv34IDQ3Fm2++iZiYGCQnJ6N79+6wtrbGhQsXsGnTJixYsAD9+/d/7H4nqhFMdbkaEel61OXz+Psy7vv374t27dqJBg0a6FzqLoQQCxYsEADExo0bhRCaS6U/++wz4e3tLeRyuQgMDBQ7duwQUVFRwtvbW7tdRZegl17qvmnTpnLrPHbsmLbN29tb9O7dW+zevVu0adNGyOVy0bx58zLbljePkBBCJCUliZdfflnUqVNHyOVy4e3tLQYMGCDi4+Mfuc9Kay9drKyshLOzs+jQoYOYNGmS9lL2BxUUFIjx48cLd3d3UatWLdGpUyeRmJgoQkNDRWhoqE7f7du3Cz8/P2FlZaVzKf3Zs2dF165dhYODg6hbt64YOXKkOHXqlE6f27dvi9GjR4vmzZsLe3t7oVQqRYcOHcS3336r8xqV3Xf61J2Xlyc++ugj0ahRI2FtbS3c3NxE//79tfMglVq2bJkICgoStWrVEo6OjqJ169bigw8+EDdu3HjkfieqSWRCGGDEIxFJWsOGDdGqVSvs2LHD1KVUO9x3RKbFMUJEREQkWQxCREREJFkMQkRERCRZJg1CsbGxaNOmDRQKBRQKBYKDg8vcSuBBcXFx2rtAly62trZVWDERlefKlSsc4/KEuO+ITMukl883aNAAs2bNQtOmTSGEwKpVq9CvXz8kJSWhZcuW5W6jUChw/vx57eOKJj0jIiIiehyTBqHSOVBKffrpp4iNjcWRI0cqDEIymazCydmIiIiI9GE2Eyqq1Wps2rQJubm5CA4OrrBfTk4OvL29UVJSgmeeeQafffZZhaEJAAoLC3VmcS29C3edOnV4NImIiKiaEELgr7/+goeHBywsDDiyx7TTGAnx22+/CXt7e2FpaSmUSqX43//+V2Hfw4cPi1WrVomkpCSRkJAg+vTpIxQKhbh69WqF20ybNu2Rk9Rx4cKFCxcuXKrP8qjf/Cdh8gkVi4qKkJ6eDpVKhe+++w7Lly/HgQMH4Ofn99hti4uL0aJFCwwaNKjce+YAZY8IqVQqeHl54erVq9q7URMREZF5y87OhqenJ7KysqBUKg32vCY/NWZjY4MmTZoA0NyA8dixY1iwYAGWLl362G2tra0RGBiIixcvVthHLpdDLpeXaS+9Uo2IiIiqD0MPazG7eYRKSkrK3Jm5Imq1GikpKXB3dzdyVURERFQTmfSI0KRJk9CzZ094eXnhr7/+wvr165GQkIDdu3cD0NxFuX79+oiJiQEAzJgxAx07dkSTJk2QlZWF2bNnIy0tDSNGjDDl2yAiIqJqyqRB6NatW4iMjERGRgaUSiXatGmD3bt3o1u3bgCA9PR0nZHh9+7dw8iRI5GZmYnatWsjKCgIhw8frtR4IiIiIqKHmXywdFXLzs6GUqmESqV65BghtVqN4uLiKqyMHmRtbQ1LS0tTl0FERGaisr/f+jL5YGlzI4RAZmYmsrKyTF2K5Dk5OcHNzY3zPRERkdEwCD2kNAS5uLjAzs6OP8ImIIRAXl4ebt26BQAcDE9EREbDIPQAtVqtDUF16tQxdTmSVqtWLQCacWQuLi48TUZEREZhdpfPm1LpmCA7OzsTV0LAP/8dOFaLiIiMhUGoHDwdZh7434GIiIyNQYiIiIgki0FIYmQyGbZt22bqMoiIiMwCg1ANkpmZiXfeeQc+Pj6Qy+Xw9PRE3759ER8fb+rSAABbtmxB9+7dUadOHchkMiQnJ5u6JCIikjgGIQPLLihGhiq/3HUZqnxkFxhn4O+VK1cQFBSEffv2Yfbs2UhJScGuXbvQpUsXjB492iivqa/c3Fz861//wueff27qUoiIiAAwCBlUdkExolYcRcTSI7iRpRuGbmTlI2LpEUStOGqUMPT2229DJpPh6NGjeOWVV+Dr64uWLVsiOjoaR44cqXC7CRMmwNfXF3Z2dvDx8cGUKVN0rtI6deoUunTpAkdHRygUCgQFBeH48eMAgLS0NPTt2xe1a9eGvb09WrZsiZ07d1b4WkOGDMHUqVPRtWtXw71xIiKip8B5hAwot/A+7uQUIf1uHgYuO4INb3SEh1Mt3MjKx8BlR5B+N0/bT2FrbbDXvXv3Lnbt2oVPP/0U9vb2ZdY7OTlVuK2joyPi4uLg4eGBlJQUjBw5Eo6Ojvjggw8AAIMHD0ZgYCBiY2NhaWmJ5ORkWFtrah89ejSKiopw8OBB2Nvb4+zZs3BwcDDY+yIiIjI2BiEDclfWwoY3OmpDz8BlRzAvwh/jNp5C+t08eDnbYcMbHeGurGXQ17148SKEEGjevLne206ePFn774YNG+K9997Dhg0btEEoPT0d77//vva5mzZtqu2fnp6OV155Ba1btwYA+Pj4PM3bICIiqnI8NWZgHk6aMOTlbIf0u3l4JTZRJwR5OBk2BAGaW1I8qY0bN6JTp05wc3ODg4MDJk+ejPT0dO366OhojBgxAl27dsWsWbNw6dIl7bp///vf+OSTT9CpUydMmzYNv/3221O9DyIioqrGIGQEHk61MC/CX6dtXoS/UUIQoDlKI5PJcO7cOb22S0xMxODBg9GrVy/s2LEDSUlJ+Oijj1BUVKTt8/HHH+PMmTPo3bs39u3bBz8/P2zduhUAMGLECFy+fBlDhgxBSkoK2rZtiy+++MKg742IiMiYGISM4EZWPsZtPKXTNm7jqTIDqA3F2dkZYWFhWLx4MXJzc8usz8rKKne7w4cPw9vbGx999BHatm2Lpk2bIi0trUw/X19fjBs3Dj/99BNefvllrFy5UrvO09MTo0aNwpYtWzB+/Hh89dVXBntfRERExsYgZGAPDoz2crbD5reCtafJBi4rezWZoSxevBhqtRrt27fH5s2bceHCBfz+++9YuHAhgoODy92madOmSE9Px4YNG3Dp0iUsXLhQe7QHAPLz8zFmzBgkJCQgLS0Nhw4dwrFjx9CiRQsAwNixY7F7926kpqbi5MmT2L9/v3Zdee7evYvk5GScPXsWAHD+/HkkJycjMzPTgHuCiIio8hiEDChDpRuCNrzREUHezjpjhgYuO1LhPENPw8fHBydPnkSXLl0wfvx4tGrVCt26dUN8fDxiY2PL3ebFF1/EuHHjMGbMGAQEBODw4cOYMmWKdr2lpSXu3LmDyMhI+Pr6YsCAAejZsyemT58OAFCr1Rg9ejRatGiBHj16wNfXF19++WWFNX7//fcIDAxE7969AQADBw5EYGAglixZYsA9QUREVHky8TQjbauh7OxsKJVKqFQqKBQKnXUFBQVITU1Fo0aNYGtrq/9z/z2P0J2cojIDo0uPFNVxsMGq19sb9PL5mupp/3sQEVHN8ajf76fBy+cNSGFrjVWvt0du4f0yl8h7ONXCxjc7wl5uxRBERERkJhiEDExha11h0DH0/EFERET0dDhGiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQUhiZDIZtm3bZuoyiIiIzAKDUA2SmZmJd955Bz4+PpDL5fD09ETfvn0RHx9v6tJQXFyMCRMmoHXr1rC3t4eHhwciIyNx48YNU5dGREQSxpmljaVEDaQdBnJuAg6ugHcIYGFptJe7cuUKOnXqBCcnJ8yePRutW7dGcXExdu/ejdGjR+PcuXNGe+3KyMvLw8mTJzFlyhT4+/vj3r17ePfdd/Hiiy/i+PHjJq2NiIiki0eEjOHs98D8VsCqPsDm4Zr/nd9K024kb7/9NmQyGY4ePYpXXnkFvr6+aNmyJaKjo3HkyJEKt5swYQJ8fX1hZ2cHHx8fTJkyBcXFxdr1p06dQpcuXeDo6AiFQoGgoCBtcElLS0Pfvn1Ru3Zt2Nvbo2XLlti5c2e5r6NUKrFnzx4MGDAAzZo1Q8eOHbFo0SKcOHEC6enpht0ZRESGUKIGUn8GUr7T/G+J2tQVkRHwiJChnf0e+DYSgNBtz87QtA9YDfi9aNCXvHv3Lnbt2oVPP/0U9vb2ZdY7OTlVuK2joyPi4uLg4eGBlJQUjBw5Eo6Ojvjggw8AAIMHD0ZgYCBiY2NhaWmJ5ORkWFtr7qU2evRoFBUV4eDBg7C3t8fZs2fh4OBQ6bpVKhVkMtkj6yMiMomz3wO7JgDZD5y+V3gAPT43+Hc4mRaDkCGVqDX/x3k4BAF/t8mAXROB5r0Neprs4sWLEEKgefPmem87efJk7b8bNmyI9957Dxs2bNAGofT0dLz//vva527atKm2f3p6Ol555RW0bt0aAODj41Pp1y0oKMCECRMwaNAgKBQKvesmIjIaE/xBS6bDU2OGlHZY96+HMgSQfV3Tz4CEKC94Vc7GjRvRqVMnuLm5wcHBAZMnT9Y5VRUdHY0RI0aga9eumDVrFi5duqRd9+9//xuffPIJOnXqhGnTpuG3336r1GsWFxdjwIABEEIgNjb2iWsnIjK4x/5BC80ftDxNVmMwCBlSzk3D9qukpk2bQiaT6T0gOjExEYMHD0avXr2wY8cOJCUl4aOPPkJRUZG2z8cff4wzZ86gd+/e2LdvH/z8/LB161YAwIgRI3D58mUMGTIEKSkpaNu2Lb744otHvmZpCEpLS8OePXt4NIiIzIuJ/qAl02EQMiQHV8P2qyRnZ2eEhYVh8eLFyM3NLbM+Kyur3O0OHz4Mb29vfPTRR2jbti2aNm2KtLS0Mv18fX0xbtw4/PTTT3j55ZexcuVK7TpPT0+MGjUKW7Zswfjx4/HVV19VWGdpCLpw4QL27t2LOnXq6P9miYiMyUR/0JLpMAgZkneIZjAdZBV0kAGK+pp+BrZ48WKo1Wq0b98emzdvxoULF/D7779j4cKFCA4OLnebpk2bIj09HRs2bMClS5ewcOFC7dEeAMjPz8eYMWOQkJCAtLQ0HDp0CMeOHUOLFi0AAGPHjsXu3buRmpqKkydPYv/+/dp1DysuLkb//v1x/PhxrFu3Dmq1GpmZmcjMzNQ5AkVEZFIm+oOWTIdByJAsLDVXFAAoG4b+ftxjllHmE/Lx8cHJkyfRpUsXjB8/Hq1atUK3bt0QHx9f4TicF198EePGjcOYMWMQEBCAw4cPY8qUKdr1lpaWuHPnDiIjI+Hr64sBAwagZ8+emD59OgBArVZj9OjRaNGiBXr06AFfX198+eWX5b7W9evX8f333+PatWsICAiAu7u7djl8mIeYichMmPAPWjINmXiakbbVUHZ2NpRKJVQqVZnxKQUFBUhNTUWjRo1ga2v75C9S7mWX9TUhiFcaVJrB/nsQEelDe9UYoDto+u9wxKvGTOJRv99Pg5fPG4Pfi5pL5KtwZmkiIjIQvxc1YafceYT4B21NwyBkLBaWQKNnTV0FERE9Cf5BKxkMQkREROWp6j9oq/gelaRh0sHSsbGxaNOmDRQKBRQKBYKDg/Hjjz8+cptNmzahefPmsLW1RevWrSu8txUREVG1YYJ7VJKGSYNQgwYNMGvWLJw4cQLHjx/H888/j379+uHMmTPl9j98+DAGDRqE4cOHIykpCeHh4QgPD8fp06cNWpfExo+bLf53ICJJKB2c/fBEjqW39GAYMiqzu2rM2dkZs2fPxvDhw8usi4iIQG5uLnbs2KFt69ixIwICArBkyZJKPf+jRp2r1Wr88ccfcHFx4WR/ZuDOnTu4desWfH19YWnJw8NEVAOVqDVHfiqczVqmGaQ9NkXyp8lq/FVjarUamzZtQm5uboUTACYmJiI6OlqnLSwsDNu2bavweQsLC1FYWKh9nJ2dXWFfS0tLODk54datWwAAOzs7yGQVzSVBxiKEQF5eHm7dugUnJyeGICKqufS5pQcvwDEKkwehlJQUBAcHo6CgAA4ODti6dSv8/PzK7ZuZmQlXV93ZPF1dXZGZmVnh88fExGgnAKwMNzc3ANCGITIdJycn7X8PIqIaibf0MDmTB6FmzZohOTkZKpUK3333HaKionDgwIEKw5C+Jk2apHMUKTs7G56enhX2l8lkcHd3h4uLC4qLiw1SA+nP2tqaR4KIqObjLT1MzuRByMbGBk2aNAEABAUF4dixY1iwYAGWLl1apq+bmxtu3tRNxTdv3nzkUQO5XA65XK53XZaWlvwhJiIi4yq9pUd2BnRnsS719xgh3tLDaMzuXmMlJSU6Y3oeFBwcjPj4eJ22PXv2VDimiIiIyKyZ8B6VpGHSIDRp0iQcPHgQV65cQUpKCiZNmoSEhAQMHjwYABAZGYlJkyZp+7/77rvYtWsX5syZg3PnzuHjjz/G8ePHMWbMGFO9BSIioqdTeksPhbtuu8KD9zWrAiY9NXbr1i1ERkYiIyMDSqUSbdq0we7du9GtWzcAQHp6Oiws/slqISEhWL9+PSZPnowPP/wQTZs2xbZt29CqVStTvQUiIiJdTzJDNG/pYTJmN4+QsRlrHgIiIiKc/b6Cm7V+ziM7T8lYv99mN0aIiIioWuIM0dUSgxAREdHTKlFrjgSVe+XX3227Jmr6kVlhECIiInpa+swQTWaFQYiIiOhpcYboaotBiIiI6Glxhuhqi0GIiIjoaZXOEF1mUsRSMkBRnzNEmyEGISIioqdVyRmiL6VeQXZ2dlVWRo/BIERERGQIj5khOtf7BbTvGAwXVzcMHvwa9u7dC7WaV5GZGidUJCIiMqRHzCw96P/+Dxu++QZWtRxxP/8vuHnUx+tDoxAVFQVfX18TF27ejPX7zSBERERV60luQVFD3LlzB81b+CFf2RCOHQcg53Q8Cs//jOL8HLRr3wHDXx+GiIgIODk5mbpUs8MgZCAMQkREJsRbUGD79u0IDw9Hnd7j4NDqBYj7Rci78CvyzuxD/uUTsLK2xkvh/TB06FB069YNVlYmvS2o2WAQMhAGISIiEym9BUWZ2Zf/HkwsoTutvzZkCDZu3gbXYYtg5VhX234/5y5yzySg4Ow+FNy6gh69++DHHT+YsFLzwXuNERFR9cVbUOj4YuFCOCsccG/3Ijx4PMLKwRnKDi/DysUHANCsaRNTlSgZDEJERGR81fEWFCVqIPVnIOU7zf8aMKTVrl0bK75ejrxLx5GbskfbLv5+jdyzB+Dl5Y3333vPYK9J5WMQIiIi46tut6A4+z0wvxWwqg+webjmf+e3Mugd5Hv37o2oqKFQ7f8a97Nv4a/kXchYNhL5l0/AZeCnyFTlonUbf/z4448Ge00qi0GIiIiMrzrdgqJ0LNPDR7CyMzTtBgxD8+fPQ53aSmSuGou7uxehkWtt3No0DfmXj8PltXkorO2DXr164YMPPkBxcbHBXpf+wSBERETGV11uQVHFY5mcnJywdvUqBLX2w4YNG/D772cxa9Ys5Bzbirvfx6B297fg1Pl1zJk7F88+F4r09HSDvC79g0GIiIiMr5K3oDD5fEImGMvUtWtX/HokEREREbCwsMCECRPw88GDUJZk49aqsbCu0wAug2Yh+dwltG7jjx9+4FVkhsQgREREVeMxt6Awi0vnzWQsU0hICFJOJaN7l+fw5+YZyD1/CPVemwO1SzO8+OKLiI6ORlFRkVFrkArO0kRERFXH70WgeW/znVnajMYy1alTBz/88D3mzZuHDyZMwP0b51C7z3uw8WqDBQu/wMGff8GmbzeiUaNGRq+lJuMRISIiqloWlkCjZ4HW/TX/ay4hCDC7sUwymQzR0dE49MsvqGORhz9Xj4WVwgUug/+D05euoU1AALZs2VIltdRUDEJERESlzHQsU4cOHfDbqWT0CuuKP7d+itwz+1Fv8GwI99Z45ZVXMGbMGBQWFlZpTTUFgxAREdGDzHQsU+3atbF1yxZ88cUXKPhtN+5sng5l6FA4dxuF2KXL0KFjMC5evGiS2qoz3muMiIioPCVqsx3LdOLECbzy6gBcz7gJp7B3YF3bHfd++A8sC1VY8fXXGDBggKlLNDjea4yIiKgqmfFYpqCgIJxKOonwvr1we/ss/HVqN+oO+hzwfAYRERF48803kZ+fb+oyqwUGISIiompIqVTi240bERsbi8Iz8bj93VQo/zUYzmFj8PXKVWjXvgPOnz9v6jLNHoMQERFRNSWTyTBq1CgcO/orPBwscWv1WFjY2MLltf/iUmYWAp8Jwrp160xdplljECIiIqrm/P39kXzyBF595SXc/uG/+OvkD6g7aBYsG7XHa6+9huHDhyMvL8/UZZolBiEiIqIawNHREevWrsXy5ctRfP5n3N74ERQdB6BOz3exau06PNO2HXJzc01dptlhECIiIqohZDIZhg8fjuPHjsKrti1urhmHkoK/YKWoh9TLlznXUDkYhIiIiGqYVq1a4eSJ43ht0EDc278CXs72OHH8GJydnU1dmtnhPEJEREQ12M6dOxEaGgp7e3tTl/JUjPX7zZuuEhER1WC9evUydQlmjafGiIiISLIYhIiIiEiyGISIiIhIshiEiIiISLIYhIiIiEiyGISIiIhIshiEiIiISLIYhIiIiEiyTBqEYmJi0K5dOzg6OsLFxQXh4eE4f/78I7eJi4uDTCbTWWxtbauoYiIiIqpJTBqEDhw4gNGjR+PIkSPYs2cPiouL0b1798feHVehUCAjI0O7pKWlVVHFREREVJOY9BYbu3bt0nkcFxcHFxcXnDhxAs8991yF28lkMri5uRm7PCIiIqrhzGqMkEqlAoDH3h03JycH3t7e8PT0RL9+/XDmzJmqKI+IiIhqGLMJQiUlJRg7diw6deqEVq1aVdivWbNmWLFiBbZv3461a9eipKQEISEhuHbtWrn9CwsLkZ2drbMQERERAYBMCCFMXQQAvPXWW/jxxx/xyy+/oEGDBpXerri4GC1atMCgQYMwc+bMMus//vhjTJ8+vUy7SqWCQqF4qpqJiIioamRnZ0OpVBr899ssjgiNGTMGO3bswP79+/UKQQBgbW2NwMBAXLx4sdz1kyZNgkql0i5Xr141RMlERERUA5h0sLQQAu+88w62bt2KhIQENGrUSO/nUKvVSElJQa9evcpdL5fLIZfLn7ZUIiIiqoFMGoRGjx6N9evXY/v27XB0dERmZiYAQKlUolatWgCAyMhI1K9fHzExMQCAGTNmoGPHjmjSpAmysrIwe/ZspKWlYcSIESZ7H0RERFQ9mTQIxcbGAgA6d+6s075y5UoMHToUAJCeng4Li3/O4N27dw8jR45EZmYmateujaCgIBw+fBh+fn5VVTYRERHVEGYzWLqqGGuwFRERERlPjR4sTURERGQKDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFkMQkRERCRZDEJEREQkWQxCREREJFl6BaH8/Hz88ssvOHv2bJl1BQUFWL16tV4vHhMTg3bt2sHR0REuLi4IDw/H+fPnH7vdpk2b0Lx5c9ja2qJ169bYuXOnXq9LRGQsKpUK165dK3fdtWvXoFKpqrgiInqUSgehP/74Ay1atMBzzz2H1q1bIzQ0FBkZGdr1KpUKw4YN0+vFDxw4gNGjR+PIkSPYs2cPiouL0b17d+Tm5la4zeHDhzFo0CAMHz4cSUlJCA8PR3h4OE6fPq3XaxMRGZpKpUKPHj0QGhqKq1ev6qy7evUqQkND0aNHD4YhIjMiE0KIynR86aWXUFxcjLi4OGRlZWHs2LE4e/YsEhIS4OXlhZs3b8LDwwNqtfqJi/nzzz/h4uKCAwcO4Lnnniu3T0REBHJzc7Fjxw5tW8eOHREQEIAlS5Y89jWys7OhVCqhUqmgUCieuFYiooddu3YNoaGhuHz5Mnx8fJCQkABPT09cvXoVnTt31rYfOHAADRo0MHW5RNWKsX6/K31E6PDhw4iJiUHdunXRpEkT/PDDDwgLC8Ozzz6Ly5cvG6SY0r+SnJ2dK+yTmJiIrl276rSFhYUhMTHRIDUQET2pBg0aICEhAT4+Prh8+TI6d+6Mw4cP64SghIQEhiAiM1LpIJSfnw8rKyvtY5lMhtjYWPTt2xehoaH4448/nqqQkpISjB07Fp06dUKrVq0q7JeZmQlXV1edNldXV2RmZpbbv7CwENnZ2ToLEZGxeHp66oShTp06lTlCRETmo9JBqHnz5jh+/HiZ9kWLFqFfv3548cUXn6qQ0aNH4/Tp09iwYcNTPc/DYmJioFQqtQu/hIjI2Dw9PbFmzRqdtjVr1vD7h8gMVToIvfTSS/jmm2/KXbdo0SIMGjQIlRxuVMaYMWOwY8cO7N+//7GHjN3c3HDz5k2dtps3b8LNza3c/pMmTYJKpdIuDw9gJCIytKtXr2LIkCE6bUOGDOH3D5EZqnQQmjRp0iMvU//yyy9RUlKi14sLITBmzBhs3boV+/btQ6NGjR67TXBwMOLj43Xa9uzZg+Dg4HL7y+VyKBQKnYWIyFgeHhh96NAhnTFDDENE5sWkEyqOHj0aa9euxfr16+Ho6IjMzExkZmYiPz9f2ycyMhKTJk3SPn733Xexa9cuzJkzB+fOncPHH3+M48ePY8yYMaZ4C0REWteuXSszMDokJKTMAOqK5hkioqpn0iAUGxsLlUqFzp07w93dXbts3LhR2yc9PV1nvqKQkBCsX78ey5Ytg7+/P7777jts27btkQOsiYiqQunksA8PjH5wALWLiwscHR1NXCkRlar0PEI1BecRIiJjUqlU+Ouvv8od73jt2jU4OjpCqVSaoDKi6s1Yv99Wj+9CRESVVXqFank4fxCR+eFNV4mIiEiynigIrVmzBp06dYKHhwfS0tIAAPPnz8f27dsNWhwRERGRMekdhGJjYxEdHY1evXohKytLe28xJycnzJ8/39D1ERERERmN3kHoiy++wFdffYWPPvoIlpaW2va2bdsiJSXFoMURERERGZPeQSg1NRWBgYFl2uVyOXJzcw1SFBEREVFV0DsINWrUCMnJyWXad+3ahRYtWhiiJiIiIqIqoffl89HR0Rg9ejQKCgoghMDRo0fxzTffICYmBsuXLzdGjURERERGoXcQGjFiBGrVqoXJkycjLy8P//d//wcPDw8sWLAAAwcONEaNREREREahVxC6f/8+1q9fj7CwMAwePBh5eXnIycmBi4uLseojIiIiMhq9xghZWVlh1KhRKCgoAADY2dkxBBEREVG1pfdg6fbt2yMpKckYtRARERFVKb3HCL399tsYP348rl27hqCgINjb2+usb9OmjcGKIyIiIjImve8+b2FR9iCSTCaDEAIymUw707S54t3niYiIqh+zuft8amqqwV6ciIiIyJT0DkLe3t7GqIOIiIioyukdhFavXv3I9ZGRkU9cDBEREVFV0nuMUO3atXUeFxcXIy8vDzY2NrCzs8Pdu3cNWqChcYwQERFR9WOs32+9L5+/d++ezpKTk4Pz58/jX//6F7755huDFUZERERkbHoHofI0bdoUs2bNwrvvvmuIpyMiIiKqEgYJQoBm1ukbN24Y6umIiIiIjE7vwdLff/+9zmMhBDIyMrBo0SJ06tTJYIURERERGZveQSg8PFznsUwmQ7169fD8889jzpw5hqqLiIiIyOj0DkIlJSXGqIOIiIioyuk9RmjGjBnIy8sr056fn48ZM2YYpCgiIiKiqqD3PEKWlpbIyMiAi4uLTvudO3fg4uLCe40RERGRwZnNPEKlN1d92KlTp+Ds7GyQooiIiIiqQqXHCNWuXRsymQwymQy+vr46YUitViMnJwejRo0ySpFERERExlDpIDR//nwIIfD6669j+vTpUCqV2nU2NjZo2LAhgoODjVIkERERkTFUOghFRUUBABo1aoSQkBBYW1sbrSgiIiKiqqD35fOhoaHafxcUFKCoqEhnPQcgExERUXWh92DpvLw8jBkzBi4uLrC3t0ft2rV1FiIiIqLqQu8g9P7772Pfvn2IjY2FXC7H8uXLMX36dHh4eGD16tXGqJGIiIjIKPQ+NfbDDz9g9erV6Ny5M4YNG4Znn30WTZo0gbe3N9atW4fBgwcbo04iIiIig9P7iNDdu3fh4+MDQDMe6O7duwCAf/3rXzh48KBhqyMiIiIyIr2DkI+PD1JTUwEAzZs3x7fffgtAc6TIycnJoMURERERGZPeQWjYsGE4deoUAGDixIlYvHgxbG1tMW7cOLz//vsGL5CIiIjIWPS+19jD0tLScOLECTRp0gRt2rQxVF1Gw3uNERERVT/G+v3We7D0gwoKCuDt7Q1vb29D1UNERERUZfQ+NaZWqzFz5kzUr18fDg4OuHz5MgBgypQp+Prrrw1eIBEREZGx6B2EPv30U8TFxeE///kPbGxstO2tWrXC8uXLDVocERERkTHpHYRWr16NZcuWYfDgwbC0tNS2+/v749y5cwYtjqg6yi4oRoYqv9x1Gap8ZBcUV3FFRERUEb2D0PXr19GkSZMy7SUlJSgu1u8L/uDBg+jbty88PDwgk8mwbdu2R/ZPSEiATCYrs2RmZur1ukTGkl1QjKgVRxGx9AhuZOmGoRtZ+YhYegRRK44yDBERmQm9g5Cfnx9+/vnnMu3fffcdAgMD9Xqu3Nxc+Pv7Y/HixXptd/78eWRkZGgXFxcXvbYnMpbcwvu4k1OE9Lt5GLjsnzB0IysfA5cdQfrdPNzJKUJu4X0TV0pERMATXDU2depUREVF4fr16ygpKcGWLVtw/vx5rF69Gjt27NDruXr27ImePXvqWwJcXFw4eSOZJXdlLWx4o6M29AxcdgTzIvwxbuMppN/Ng5ezHTa80RHuylqmLpWIiPAER4T69euHH374AXv37oW9vT2mTp2K33//HT/88AO6detmjBrLCAgIgLu7O7p164ZDhw49sm9hYSGys7N1FiJj8nDShCEvZzuk383DK7GJOiHIw4khiIjIXFQ6CF2+fBmlcy8+++yz2LNnD27duoW8vDz88ssv6N69u9GKLOXu7o4lS5Zg8+bN2Lx5Mzw9PdG5c2ecPHmywm1iYmKgVCq1i6enp9HrJPJwqoV5Ef46bfMi/BmCiIjMTKVnlra0tNQZjxMREYGFCxfC1dXVMIXIZNi6dSvCw8P12i40NBReXl5Ys2ZNuesLCwtRWFiofZydnQ1PT0/OLE1G9eCYoFI8IkRE9OSMNbN0pY8IPZyXdu7cidzcXIMV8qTat2+PixcvVrheLpdDoVDoLETG9GAI8nK2w+a3grWnyR4cQE1ERKan9xghc5OcnAx3d3dTl0EEQDNP0IMhaMMbHRHk7awzZmjgsiMVzjNERERVq9JXjZXO2fNw29PIycnROZqTmpqK5ORkODs7w8vLC5MmTcL169exevVqAMD8+fPRqFEjtGzZEgUFBVi+fDn27duHn3766anqIDIUe7kV6jhoZlx/8DRY6QDqgcuOoI6DDezlT3WbPyIiMpBKfxsLITB06FDI5XIAmhuujho1Cvb29jr9tmzZUukXP378OLp06aJ9HB0dDQCIiopCXFwcMjIykJ6erl1fVFSE8ePH4/r167Czs0ObNm2wd+9enecgMiWFrTVWvd4euYX3y1wi7+FUCxvf7Ah7uRUUttYmqpCIiB5U6cHSw4YNq9QTrly58qkKMjZjDbYiIiIi4zHW73eljwiZe8AhIiIi0le1HyxNRERE9KQYhIiIiEiyGISIiIhIshiEiIiISLIYhIiIiEiyGISIiIhIshiEiIiISLIYhIiIiEiyGISIzER2QXGFN2PNUOUju6C4iisiIqr5GISIzEB2QTGiVhxFxNIjuJGlG4ZuZOUjYukRRK04yjBERGRgDEJEZiC38D7u5BQh/W4eBi77JwzdyMrHwGVHkH43D3dyipBbeN/ElRIR1SwMQkRmwF1ZCxve6AgvZzttGDqRdlcbgryc7bDhjY5l7mhPRERPp9J3n68pePd5MmcPHgEqVRqCPJwYgohIuoz1+80jQkRmxMOpFuZF+Ou0zYvwZwgiIjISBiEiM3IjKx/jNp7SaRu38VSZAdRERGQYDEJEZuLB02JeznbY/FawzpghhiEiIsNjECIyAxmq/DIDo4O8ncsMoK5oniEiInoyDEJEZsBeboU6DjZlBkZ7OP1zNVkdBxvYy61MXCkRUc3Cq8aIzER2QTFyC++Xe4l8hiof9nIrKGytTVAZEZHpGev3m39eEpkJha11hUGH8wcRERkHT40RERGRZDEIERERkWQxCBEREZFkMQgRERGRZDEIUbWRXVBc4Tw6Gap8ZBcUV3FFRERU3TEIUbWQXVCMqBVHEbG07AzLN7LyEbH0CKJWHGUYIiIivTAIUbWQW3gfd3KKytxu4sHbUtzJKUJu4X0TV0pERNUJgxBVC+7KWmVuN3Ei7W6Z21Jwvh0iItIHZ5amauXBI0ClHr4tBRER1TzG+v3mESGqVjycamFehL9O27wIf4YgIiJ6IgxCVK3cyMrHuI2ndNrGbTxVZgA1ERFRZTAIUbXx4GkxL2c7bH4rWGfMEMMQERHpi0GIqoUMVX6ZgdFB3s5lBlBXNM8QERFReRiEqFqwl1uhjoNNmYHRHk7/XE1Wx8EG9nIrE1dKRETVCa8ao2oju6AYuYX3y71EPkOVD3u5FRS21iaojIiIjM1Yv9/885mqDYWtdYVBh/MHERHRk+CpMSIiIpIsBiEiIiKSLAYhIiIikiwGISIiIpIsDpYmqsBTX6VWogbSDgM5NwEHV8A7BLCwNGLFRESkL5MeETp48CD69u0LDw8PyGQybNu27bHbJCQk4JlnnoFcLkeTJk0QFxdn9DpJerILihG14igilpadsfpGVj4ilh5B1IqjyC4oLv8Jzn4PzG8FrOoDbB6u+d/5rTTtRERkNkwahHJzc+Hv74/FixdXqn9qaip69+6NLl26IDk5GWPHjsWIESOwe/duI1dKUpNbeB93corK3L7jwdt83MkpQm7h/bIbn/0e+DYSyL6h256doWlnGCIiMhtmM6GiTCbD1q1bER4eXmGfCRMm4H//+x9Onz6tbRs4cCCysrKwa9euSr0OJ1Skynr43mbzIvwxbuMpndt8lLnrfYlac+Tn4RCkJQMUHsDYFJ4mIyLSg7F+v6vVYOnExER07dpVpy0sLAyJiYkVblNYWIjs7GydhagyHrx9R/rdPLwSm/joEARoxgRVGIIAQADZ1zX9iIjI5KpVEMrMzISrq6tOm6urK7Kzs5GfX/7NNmNiYqBUKrWLp6dnVZRKNYSHUy3Mi/DXaZsX4V9+CAI0A6Mro7L9iIjIqKpVEHoSkyZNgkql0i5Xr141dUlUjdzIyse4jad02sZtPFVmALWWg2v57U/aj4iIjKpaBSE3NzfcvKn7l/TNmzehUChQq1b5f6HL5XIoFAqdhagyHh4jtPmtYO1psgcHUOvwDtGMAYKsgmeVAYr6mn5ERGRy1SoIBQcHIz4+Xqdtz549CA4ONlFFVFNlqHRD0IY3OiLI21lnzNDAZUeQoXooDFlYAj0+//vBw2Ho78c9ZgH/nQNs3AgUFBj7rRAR0SOYNAjl5OQgOTkZycnJADSXxycnJyM9PR2A5rRWZGSktv+oUaNw+fJlfPDBBzh37hy+/PJLfPvttxg3bpwpyqcazF5uhToONmUGRj84gLqOgw3s5eXMSer3IjBgNaBw121XeGjaa7cDPvwQGDgQcHEB3nwTSEwEzOMCTiIiSTHp5fMJCQno0qVLmfaoqCjExcVh6NChuHLlChISEnS2GTduHM6ePYsGDRpgypQpGDp0aKVfk5fPU2UZdWbpGTOAadM0/7ayAu7fB3x8gOHDgSFDAA7qJyLSYazfb7OZR6iqMAiRWSguBoKCgLNnAbX6n3YLC82Roc6dgddfB15+GbCzM1mZRETmgvMIEdUk1tbAunWA7KFxRCUlmiB04IDmyFC9eppA9PfpYyIiMiwGISJTad0amD69bBgCNIEIAPLygJUrgcmTq7Y2IiKJYBAiMqUPPgD8/QHLCm63YWkJeHgAU6ZUbV1ERBLBIERkSlZW5Z8ie1B2NnDtWtXVREQkIQxCRKbm5wd8+uk/jx88OqRWA7m5QP/+wOjRnHeIiMjAGISIzMH48ZqryACgfn1g6lTN0SJLy3/mF1qyBGjfHrh40XR1EhHVMAxCRObA0hLYsgWIjQV++00ziPrIEaBBg3+OEJWUaC639/cHNmwwbb1ERDUEgxCRufDyAkaNApRKzeOgIE0oeumlf/qo1UB+PjBoEPDGG5p/ExHRE2MQIjJnCgXw7bea02LW1rqnyr7+GmjbFjh/3rQ1EhFVYwxCROZOJtPcj+zYMcDbWzP7NKA5VXb+PBAQAKxda9ISiYiqKwYhourC318zw3RExD9tarXmSrIhQ4BhwzQTMBIRUaUxCBFVJ46OmnmHvv4akMt1L7VfvRoIDATOnDFdfURE1QyDEFF1I5Np7j92/LjmjvUPniq7dEkzyHrlyn/GEhERUYUYhIiqq1atgKQkIDLynza1Gigs1ASlIUOAnBzT1UdEVA0wCBFVZ/b2mqM/q1cDtra6p8q++UZzquy330xXHxGRmWMQIqoJhgzRHB3y9dU9VZaaCrRrByxbxlNlRETlYBAiqimaNwdOnNCcFiulVgNFRZrL7wcO1NzAlYiItBiEiGqSWrWAr74C1q/X/PvBU2WbN2suwU9KMl19RERmhkGIqCYaNAg4dQpo0eKfU2VqNZCeDnTooDltRkREDEJENVbTpprZqN9885+2khLg+ec1l+ATERGDEFGNZmsLfPklsGkT4OwMzJ4N7NzJIERE9DcrUxdARFWgf3+gb1/NbNRERKTFI0JEUsEQRERUBoMQERERSRaDEBEREUkWgxARERFJFoMQERERSRaDEBEREUkWgxARERFJFoMQERERSRaDEBEREUkWgxARERFJFoMQERERSRaDEBEREUkWgxARERFJFoMQERERSRaDEBEREUkWgxARERFJFoMQERERSRaDEBEREUkWgxARERFJFoMQERERSZZZBKHFixejYcOGsLW1RYcOHXD06NEK+8bFxUEmk+kstra2VVgtERER1RQmD0IbN25EdHQ0pk2bhpMnT8Lf3x9hYWG4detWhdsoFApkZGRol7S0tCqsmIiIiGoKkwehuXPnYuTIkRg2bBj8/PywZMkS2NnZYcWKFRVuI5PJ4Obmpl1cXV2rsGIiIiKqKUwahIqKinDixAl07dpV22ZhYYGuXbsiMTGxwu1ycnLg7e0NT09P9OvXD2fOnKmwb2FhIbKzs3UWIiIiIsDEQej27dtQq9Vljui4uroiMzOz3G2aNWuGFStWYPv27Vi7di1KSkoQEhKCa9eulds/JiYGSqVSu3h6ehr8fRAREVH1ZPJTY/oKDg5GZGQkAgICEBoaii1btqBevXpYunRpuf0nTZoElUqlXa5evVrFFRMREZG5sjLli9etWxeWlpa4efOmTvvNmzfh5uZWqeewtrZGYGAgLl68WO56uVwOuVz+1LUSERFRzWPSI0I2NjYICgpCfHy8tq2kpATx8fEIDg6u1HOo1WqkpKTA3d3dWGUSERFRDWXSI0IAEB0djaioKLRt2xbt27fH/PnzkZubi2HDhgEAIiMjUb9+fcTExAAAZsyYgY4dO6JJkybIysrC7NmzkZaWhhEjRpjybRAREVE1ZPIgFBERgT///BNTp05FZmYmAgICsGvXLu0A6vT0dFhY/HPg6t69exg5ciQyMzNRu3ZtBAUF4fDhw/Dz8zPVWyAiIqJqSiaEEKYuoiplZ2dDqVRCpVJBoVCYuhwiIiKqBGP9fle7q8aIiIiIDIVBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCSLQYiIiIgki0GIiIiIJItBiIiIiCTLLILQ4sWL0bBhQ9ja2qJDhw44evToI/tv2rQJzZs3h62tLVq3bo2dO3dWUaVERERUk5g8CG3cuBHR0dGYNm0aTp48CX9/f4SFheHWrVvl9j98+DAGDRqE4cOHIykpCeHh4QgPD8fp06eruHIiIiKq7mRCCGHKAjp06IB27dph0aJFAICSkhJ4enrinXfewcSJE8v0j4iIQG5uLnbs2KFt69ixIwICArBkyZLHvl52djaUSiVUKhUUCoXh3ggREREZjbF+v016RKioqAgnTpxA165dtW0WFhbo2rUrEhMTy90mMTFRpz8AhIWFVdifiIiIqCJWpnzx27dvQ61Ww9XVVafd1dUV586dK3ebzMzMcvtnZmaW27+wsBCFhYXaxyqVCoAmWRIREVH1UPq7begTWSYNQlUhJiYG06dPL9Pu6elpgmqIiIjoady5cwdKpdJgz2fSIFS3bl1YWlri5s2bOu03b96Em5tbudu4ubnp1X/SpEmIjo7WPs7KyoK3tzfS09MNuiNrguzsbHh6euLq1ascP/UQ7puKcd9UjPumYtw35eN+qZhKpYKXlxecnZ0N+rwmDUI2NjYICgpCfHw8wsPDAWgGS8fHx2PMmDHlbhMcHIz4+HiMHTtW27Znzx4EBweX218ul0Mul5dpVyqV/JBVQKFQcN9UgPumYtw3FeO+qRj3Tfm4XypmYWHY4c0mPzUWHR2NqKgotG3bFu3bt8f8+fORm5uLYcOGAQAiIyNRv359xMTEAADeffddhIaGYs6cOejduzc2bNiA48ePY9myZaZ8G0RERFQNmTwIRURE4M8//8TUqVORmZmJgIAA7Nq1SzsgOj09XSf9hYSEYP369Zg8eTI+/PBDNG3aFNu2bUOrVq1M9RaIiIiomjJ5EAKAMWPGVHgqLCEhoUzbq6++ildfffWJXksul2PatGnlni6TOu6binHfVIz7pmLcNxXjvikf90vFjLVvTD6hIhEREZGpmPwWG0RERESmwiBEREREksUgRERERJLFIERERESSVSOD0OLFi9GwYUPY2tqiQ4cOOHr06CP7b9q0Cc2bN4etrS1at26NnTt3VlGlVU+ffRMXFweZTKaz2NraVmG1VefgwYPo27cvPDw8IJPJsG3btsduk5CQgGeeeQZyuRxNmjRBXFyc0eusavrul4SEhDKfGZlMVuG9AKuzmJgYtGvXDo6OjnBxcUF4eDjOnz//2O2k8H3zJPtGKt83sbGxaNOmjXbCxODgYPz444+P3EYKnxl994shPy81Lght3LgR0dHRmDZtGk6ePAl/f3+EhYXh1q1b5fY/fPgwBg0ahOHDhyMpKQnh4eEIDw/H6dOnq7hy49N33wCa2U0zMjK0S1paWhVWXHVyc3Ph7++PxYsXV6p/amoqevfujS5duiA5ORljx47FiBEjsHv3biNXWrX03S+lzp8/r/O5cXFxMVKFpnPgwAGMHj0aR44cwZ49e1BcXIzu3bsjNze3wm2k8n3zJPsGkMb3TYMGDTBr1iycOHECx48fx/PPP49+/frhzJkz5faXymdG3/0CGPDzImqY9u3bi9GjR2sfq9Vq4eHhIWJiYsrtP2DAANG7d2+dtg4dOog333zTqHWagr77ZuXKlUKpVFZRdeYDgNi6desj+3zwwQeiZcuWOm0REREiLCzMiJWZVmX2y/79+wUAce/evSqpyZzcunVLABAHDhyosI+Uvm8eVJl9I9XvGyGEqF27tli+fHm566T6mRHi0fvFkJ+XGnVEqKioCCdOnEDXrl21bRYWFujatSsSExPL3SYxMVGnPwCEhYVV2L+6epJ9AwA5OTnw9vaGp6fnY9O5lEjlc/OkAgIC4O7ujm7duuHQoUOmLqdKqFQqAHjkDSGl+rmpzL4BpPd9o1arsWHDBuTm5lZ4v0wpfmYqs18Aw31ealQQun37NtRqtfb2HKVcXV0rHKOQmZmpV//q6kn2TbNmzbBixQps374da9euRUlJCUJCQnDt2rWqKNmsVfS5yc7ORn5+vomqMj13d3csWbIEmzdvxubNm+Hp6YnOnTvj5MmTpi7NqEpKSjB27Fh06tTpkbf7kcr3zYMqu2+k9H2TkpICBwcHyOVyjBo1Clu3boWfn1+5faX0mdFnvxjy82IWt9gg8xQcHKyTxkNCQtCiRQssXboUM2fONGFlZK6aNWuGZs2aaR+HhITg0qVLmDdvHtasWWPCyoxr9OjROH36NH755RdTl2J2KrtvpPR906xZMyQnJ0OlUuG7775DVFQUDhw4UOGPvlTos18M+XmpUUGobt26sLS0xM2bN3Xab968CTc3t3K3cXNz06t/dfUk++Zh1tbWCAwMxMWLF41RYrVS0edGoVCgVq1aJqrKPLVv375GB4QxY8Zgx44dOHjwIBo0aPDIvlL5vimlz755WE3+vrGxsUGTJk0AAEFBQTh27BgWLFiApUuXlukrpc+MPvvlYU/zealRp8ZsbGwQFBSE+Ph4bVtJSQni4+MrPM8YHBys0x8A9uzZ88jzktXRk+ybh6nVaqSkpMDd3d1YZVYbUvncGEJycnKN/MwIITBmzBhs3boV+/btQ6NGjR67jVQ+N0+ybx4mpe+bkpISFBYWlrtOKp+Z8jxqvzzsqT4vBhlybUY2bNgg5HK5iIuLE2fPnhVvvPGGcHJyEpmZmUIIIYYMGSImTpyo7X/o0CFhZWUl/vvf/4rff/9dTJs2TVhbW4uUlBRTvQWj0XffTJ8+XezevVtcunRJnDhxQgwcOFDY2tqKM2fOmOotGM1ff/0lkpKSRFJSkgAg5s6dK5KSkkRaWpoQQoiJEyeKIUOGaPtfvnxZ2NnZiffff1/8/vvvYvHixcLS0lLs2rXLVG/BKPTdL/PmzRPbtm0TFy5cECkpKeLdd98VFhYWYu/evaZ6C0bz1ltvCaVSKRISEkRGRoZ2ycvL0/aR6vfNk+wbqXzfTJw4URw4cECkpqaK3377TUycOFHIZDLx008/CSGk+5nRd78Y8vNS44KQEEJ88cUXwsvLS9jY2Ij27duLI0eOaNeFhoaKqKgonf7ffvut8PX1FTY2NqJly5bif//7XxVXXHX02Tdjx47V9nV1dRW9evUSJ0+eNEHVxld62ffDS+n+iIqKEqGhoWW2CQgIEDY2NsLHx0esXLmyyus2Nn33y+effy4aN24sbG1thbOzs+jcubPYt2+faYo3svL2CwCdz4FUv2+eZN9I5fvm9ddfF97e3sLGxkbUq1dPvPDCC9ofeyGk+5nRd78Y8vMiE0II/Y8jEREREVV/NWqMEBEREZE+GISIiIhIshiEiIiISLIYhIiIiEiyGISIiIhIshiEiIiISLIYhIiIiEiyGISIiIhIshiEiMighg4dCplMVmYx1M0z4+Li4OTkZJDnelIHDx5E37594eHhAZlMhm3btpm0HiJ6cgxCRGRwPXr0QEZGhs7yJDfeNLbi4uIn2i43Nxf+/v5YvHixgSsioqrGIEREBieXy+Hm5qazWFpaAgC2b9+OZ555Bra2tvDx8cH06dNx//597bZz585F69atYW9vD09PT7z99tvIyckBACQkJGDYsGFQqVTaI00ff/wxAJR7ZMbJyQlxcXEAgCtXrkAmk2Hjxo0IDQ2Fra0t1q1bBwBYvnw5WrRoAVtbWzRv3hxffvnlI99fz5498cknn+Cll14ywN4iIlOyMnUBRCQdP//8MyIjI7Fw4UI8++yzuHTpEt544w0AwLRp0wAAFhYWWLhwIRo1aoTLly/j7bffxgcffIAvv/wSISEhmD9/PqZOnYrz588DABwcHPSqYeLEiZgzZw4CAwO1YWjq1KlYtGgRAgMDkZSUhJEjR8Le3h5RUVGG3QFEZH6e7n6xRES6oqKihKWlpbC3t9cu/fv3F0II8cILL4jPPvtMp/+aNWuEu7t7hc+3adMmUadOHe3jlStXCqVSWaYfALF161adNqVSqb3jeWpqqgAg5s+fr9OncePGYv369TptM2fOFMHBwY97qxW+LhFVHzwiREQG16VLF8TGxmof29vbAwBOnTqFQ4cO4dNPP9WuU6vVKCgoQF5eHuzs7LB3717ExMTg3LlzyM7Oxv3793XWP622bdtq/52bm4tLly5h+PDhGDlypLb9/v37UCqVT/1aRGT+GISIyODs7e3RpEmTMu05OTmYPn06Xn755TLrbG1tceXKFfTp0wdvvfUWPv30Uzg7O+OXX37B8OHDUVRU9MggJJPJIITQaStvMHRpKCutBwC++uordOjQQadf6ZgmIqrZGISIqMo888wzOH/+fLkhCQBOnDiBkpISzJkzBxYWmms5vv32W50+NjY2UKvVZbatV68eMjIytI8vXLiAvLy8R9bj6uoKDw8PXL58GYMHD9b37RBRDcAgRERVZurUqejTpw+8vLzQv39/WFhY4NSpUzh9+jQ++eQTNGnSBMXFxfjiiy/Qt29fHDp0CEuWLNF5joYNGyInJwfx8fHw9/eHnZ0d7Ozs8Pzzz2PRokUIDg6GWq3GhAkTYG1t/diapk+fjn//+99QKpXo0aMHCgsLcfz4cdy7dw/R0dHlbpOTk6MzL1JqaiqSk5Ph7OwMLy+vp9tJRFS1TD1IiYhqlqioKNGvX78K1+/atUuEhISIWrVqCYVCIdq3by+WLVumXT937lzh7u4uatWqJcLCwsTq1asFAHHv3j1tn1GjRok6deoIAGLatGlCCCGuX78uunfvLuzt7UXTpk3Fzp07yx0snZSUVKamdevWiYCAAGFjYyNq164tnnvuObFly5YK38P+/fsFgDJLVFSUHnuKiMyBTIiHTqoTERERSQQnVCQiIiLJYhAiIiIiyWIQIiIiIsliECIiIiLJYhAiIiIiyWIQIiIiIsliECIiIiLJYhAiIiIiyWIQIiIiIsliECIiIiLJYhAiIiIiyWIQIiIiIsn6fwP24TtPy8RFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example dataspace visualization\n",
    "from matplotlib import pyplot as plt\n",
    "c1 = np.array([[1,1],[1.1,1.5],[1.2,1.3],[0.8,1.15]])\n",
    "c2 = np.array([[3,3],[2.8,3.1],[2.9,2.9],[1.1,1],[2.5,2.8],[2.8,2.76]])\n",
    "\n",
    "plt.scatter(c1[:,0],c1[:,1], marker = 'x', label='Class 1')\n",
    "plt.scatter(c2[:,0],c2[:,1], marker='o', label='Class 2')\n",
    "plt.scatter(2,2, marker = 'x', color ='k')\n",
    "plt.xlim([0,3.5])\n",
    "plt.ylim([0,3.5])\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title(\"Example Dataspace\")\n",
    "plt.arrow(1.35, 0.75, -.15, .15, width=0.02, color = 'r')\n",
    "plt.arrow(3.15, 2.65, -.15, .15, width=0.02)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3a350-b823-4911-80d7-489e1acdcd6b",
   "metadata": {},
   "source": [
    "So, a d-knn can estimate how informative a label at a point is given the point's neighbors and thus estimate which labels are most important to keep in a training set and which ones an be discarded. However, a vanilla knn will always return $I(y_i|x_i) = 0$ for all $(x_i,y_i)$ in its training set. That is what \"self-drop\" adresses. We provide a custom weights function ('fns._src._dist_weight_ignore_self') which assigns a weight of $0$ to neighbors with $d=0$ instead of letting them determine the outcome of the classifier. This lets us estimate the amount of infrmation a trainig set has about itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acaab68d-e9e7-4c6c-ba3e-e7af13ef46ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information in the training set from frqunecy stats: 30778.913\n",
      "Information remaining in train set for vanilla dknn classifier: 0.000\n",
      "Estimated training set self-information: 6740.530\n",
      "Information remaining in train set for fully-trained classifier: 6907.605\n"
     ]
    }
   ],
   "source": [
    "train_set_self_info = btv.core.estimate_info(X_train,y_train)\n",
    "clf_knn = btv.core.fit_dknn_toXy(X_train,y_train)\n",
    "knn_trained_info = btv.core.prediction_info(y_train, clf_knn.predict_proba(X_train)).sum()\n",
    "\n",
    "print(\n",
    "    f\"Information in the training set from frqunecy stats: {frequnecy_train_info:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Information remaining in train set for vanilla dknn classifier: {knn_trained_info:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Estimated training set self-information: {train_set_self_info:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Information remaining in train set for fully-trained classifier: {full_trained_train_info:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c01036-a3aa-4302-925b-081b4e58e5bf",
   "metadata": {},
   "source": [
    "We can see that the information estimated by th self-drop knn is close to information remaining in the training set for the RF classifier, $6740$ vs. $6907$ bits out of $30,\\!778$ bits, a difference of $\\approx0.5$%.\n",
    "\n",
    "If this is a good estimation, we can exclude points with low information from the training set and retain performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7521910e-dff4-4063-9e03-ad2b9046f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score when trained on full training set: : 0.81092\n",
      "Length of pruned training set is 23480 samples, 76.29% of the full training set.\n",
      "Score when trained on pruned set: : 0.81442\n"
     ]
    }
   ],
   "source": [
    "idx_pruned = btv.selection.prune_by_info(X_train,y_train, thresh=0) # setting threshold to 0 bits, just to be safe\n",
    "clf.fit(X_train[idx_pruned],y_train[idx_pruned])\n",
    "pruned_score = clf.score(X_test,y_test)\n",
    "\n",
    "print(f\"Score when trained on full training set: : {full_train_score:0.5f}\")\n",
    "print(f\"Length of pruned training set is {len(idx_pruned)} samples, {len(idx_pruned)/len(y_train) * 100:0.2f}% of the full training set.\")\n",
    "print(f\"Score when trained on pruned set: : {pruned_score:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f5436-463a-4daa-8909-b4b509463f72",
   "metadata": {},
   "source": [
    "NB: With the provided data, the pruned set, the RF classifier perfomed **better**, $0.81442$ vs $0.81092$.\n",
    "\n",
    "Now we have a few tools to examine and compare data sets:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ead1e4-047f-47ab-b007-a17428b7a64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89e557-7ddf-4f22-aa19-ce5f3f30294e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bec09-ffe4-4a60-955a-c12fbc119c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b5a27f73-fd2c-4d37-bb38-1148b1820b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information remaining in train set for fully-trained RF classifier: 6907.605\n",
      "RF test score: : 0.81092\n",
      "Information remaining in train set for dknn classifier: 0.000\n",
      "d-knn test score: 0.809357\n"
     ]
    }
   ],
   "source": [
    "clf_knn = btv.core.fit_dknn_toXy(X_train,y_train)\n",
    "knn_trained_info = btv.core.prediction_info(y_train, clf_knn.predict_proba(X_train)).sum()\n",
    "dknn_score = clf_knn.score(X_test,y_test)\n",
    "print(\n",
    "    f\"Information remaining in train set for fully-trained RF classifier: {full_trained_train_info:.3f}\"\n",
    ")\n",
    "print(f\"RF test score: : {full_train_score:0.5f}\")\n",
    "print(\n",
    "    f\"Information remaining in train set for dknn classifier: {knn_trained_info:.3f}\"\n",
    ")\n",
    "print(f\"d-knn test score: {dknn_score:f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c05aba-8348-4d55-8663-2f9a23c5e2fa",
   "metadata": {},
   "source": [
    "The amount of information left in or taken out of a training et by a classifier raises the question of what proportion of the information in the training set a classifier extracts, the *extraction rate*, $$r = \\frac{I^* - \\sum_iI(y_i|x_i)}{I^*},$$ that is the information learned from the dataset, divided by the a priori information, $I^*$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "21a28951-7d7c-4f45-a8c7-1d89b19e8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of information learned from train set by minimum-trained classifier: 1.00000\n",
      "Fraction of information learned from train set by tenth-trained classifier: 0.77871\n",
      "Fraction of information learned from train set by fully-trained classifier: 0.77557\n"
     ]
    }
   ],
   "source": [
    "full_trained_train_info_rate = btv.core.model_extraction_rate(clf, X_train, y_train)\n",
    "tenth_trained_train_info_rate = btv.core.model_extraction_rate(\n",
    "    clf, X_train[: len(y_train) // 10], y_train[: len(y_train) // 10]\n",
    ")\n",
    "min_trained_train_info_rate = btv.core.model_extraction_rate(clf, X_min, y_min)\n",
    "\n",
    "print(\n",
    "    f\"Fraction of information learned from train set by minimum-trained classifier: {min_trained_train_info_rate:.5f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Fraction of information learned from train set by tenth-trained classifier: {tenth_trained_train_info_rate:.5f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Fraction of information learned from train set by fully-trained classifier: {full_trained_train_info_rate:.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b85c26-f802-4b9d-9767-5a5b7b55b09c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Training Data Selection\n",
    "So Shnnon information is behaving as expected, giving a quanitative measure of how ignorant a classifier is about certain set of labels. If we are in the position of having to chose a training set however, information alone is not enough. Information a set contains for a given classifier scales with its size, while performance (and information remaining in a test set) once trained does not. What is more, ideally, we would select training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da778ff-9b35-4054-9a9a-26085ebf7c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8424f1-60a6-4376-aeed-0d656f78401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosing between fine-tuning sets\n",
    "ds = data_tab.getdata(44156, verbose=False)\n",
    "df, X, y = data_tab.dataset2df(ds, class_cols=[\"class\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0, max_depth=1, random_state=10\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_dev1, y_train, y_dev1 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev2, y_train, y_dev2 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev3, y_train, y_dev3 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev4, y_train, y_dev4 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "X_train, X_dev5, y_train, y_dev5 = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "X_ft_list = [X_dev1, X_dev2, X_dev3, X_dev4, X_dev5]\n",
    "y_ft_list = [y_dev1, y_dev2, y_dev3, y_dev4, y_dev5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
