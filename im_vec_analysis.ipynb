{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b4f84c-1266-4ad0-af31-778097103f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,  # use X = np.zeros(n_samples) in .split\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "\n",
    "from data_img import Img_VAE_Dataset\n",
    "\n",
    "# if you have indeces and want knns from large set, can make an idx array and NearestVectorCaller._call_vec_set; can just train/test split np.arange(len(ds)) to get back indeces for large sets\n",
    "\n",
    "\n",
    "from btv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34820869-08c5-4797-b66c-039cc3b7d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detemining all classes...: 100%|██████████████████████████████| 1766/1766 [00:00<00:00, 2084.45it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Img_VAE_Dataset' object has no attribute 'whole_img_vec_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m#, metric = \"cosine\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# X, y = ds.whole_img_vec_set[:] # only need y for class balancing... ds.whole_img_vec_set.y\u001b[39;00m\n\u001b[1;32m      8\u001b[0m idx_train, idx_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m----> 9\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhole_img_vec_set\u001b[49m)), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m (X_train, y_train), (X_test, y_test) \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mwhole_img_vec_set[idx_train], ds\u001b[38;5;241m.\u001b[39mwhole_img_vec_set[idx_test]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Img_VAE_Dataset' object has no attribute 'whole_img_vec_set'"
     ]
    }
   ],
   "source": [
    "## Data setup\n",
    "\n",
    "ds = Img_VAE_Dataset(\"data/hardhat/test\", max_dim=128)\n",
    "# can pass metric == \"cosine\", get from NearestVectorCaller.metric on large sets\n",
    "knn = KNeighborsClassifier(weights=\"distance\", n_neighbors=10) #, metric = \"cosine\")\n",
    "# X, y = ds.whole_img_vec_set[:] # only need y for class balancing... ds.whole_img_vec_set.y\n",
    "\n",
    "idx_train, idx_test = train_test_split(\n",
    "    np.arange(len(ds)), test_size=0.2\n",
    ")\n",
    "(X_train, y_train), (X_test, y_test) = ds.whole_img_vec_set[idx_train], ds.whole_img_vec_set[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6abb3ed6-794e-4bcc-8b4b-c3cb604ff6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=10, weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=10, weights=&#x27;distance&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10, weights='distance')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "# ds.whole_img_vec_set.X[idx_test]\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86b70ff0-5ef1-46f5-91e8-fe456f36f775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8107344632768362"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(\n",
    "    ds.whole_img_vec_set.y[idx_test], knn.predict(ds.whole_img_vec_set.X[idx_test])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c387d5e-a9f9-45c1-976b-b4e8e1c0208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only include p if y==1 or, reeally, want p(right answer) so take elemnt of predicted prob indicated by binary y.\n",
    "def prediction_entropy(y_true, y_predicted): # multilable\n",
    "    # replace log(0) with n instead of -Inf\n",
    "    if isinstance(y_predicted, list):\n",
    "        y_predicted = np.stack(y_predicted, axis=1)\n",
    "    A = np.zeros_like(y_true)\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        for i, (pred, true) in enumerate(zip(y_predicted, y_true)):\n",
    "            for j, (p, t) in enumerate(zip(pred, true)):\n",
    "                I = -np.log(p[t])\n",
    "                if np.isinf(I):\n",
    "                    I = y_true.size\n",
    "                A[i, j] = I\n",
    "    return np.mean(A, axis=0)\n",
    "\n",
    "\n",
    "# probably actually want to do this at the prediction level since that is the big/slow bit.\n",
    "def prediction_entropy_lg(y_true, y_predicted): # sinlge label\n",
    "    # replace log(0) with n instead of -Inf\n",
    "    if isinstance(y_predicted, list):\n",
    "        y_predicted = np.stack(y_predicted, axis=1)\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        for pred, true in zip(y_predicted, y_true):\n",
    "            for p, t in zip(pred, true):\n",
    "                I = -np.log(p[t])\n",
    "                if np.isinf(I):\n",
    "                    I = y_true.size\n",
    "                yield I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "468bdc54-1880-48a3-a147-f64342d080a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = knn.predict(ds.whole_img_vec_set.X[idx_test])\n",
    "P = knn.predict_proba(\n",
    "    ds.whole_img_vec_set.X[idx_test]\n",
    ")  # predicts the probability of each class. For whole image hardhats it is a list of 3 (p(no), P(yes)) arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ed5e5ed-aacd-4a1f-b2c0-b1198091d312",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprediction_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_predicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/better-than-vibes/btv.py:36\u001b[0m, in \u001b[0;36mprediction_entropy\u001b[0;34m(y_true, y_predicted)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_predicted, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     35\u001b[0m     y_predicted \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(y_predicted, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m class_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43my_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     37\u001b[0m y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,class_num])\n\u001b[1;32m     38\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m y_predicted\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,class_num,\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "prediction_entropy(y_predicted, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b38014cc-98ab-46c1-98a7-8ae41a3c4902",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_idx_gen = StratifiedKFold(n_splits=5).split(\n",
    "    np.zeros(len(ds.whole_img_vec_set)), np.sum(ds.whole_img_vec_set.y, axis=1)\n",
    ")\n",
    "# can just add all the labels to make it multiclass for splitting\n",
    "entropies = []\n",
    "running_train = []\n",
    "for k, (train_idx, test_idx) in enumerate(kfold_idx_gen):\n",
    "    running_train.append(test_idx)\n",
    "    # knn.fit(ds.whole_img_vec_set.X[train_idx], ds.whole_img_vec_set.y[train_idx])\n",
    "    # y_predicted = knn.predict_proba(ds.whole_img_vec_set.X[test_idx])\n",
    "    # y_true = ds.whole_img_vec_set.y[test_idx]\n",
    "    # H = prediction_entropy(y_true, y_predicted)\n",
    "    # entropies.append(H)\n",
    "    # print(f\"Fold : {k+1}, \" f\"Fold entropy : {H}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02a465-f1c2-4835-a1ac-14c98a5dd875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f804cbeb-a4aa-4d4e-b430-3d8e53ad84cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([75.04532578, 57.61076487, 16.80113314]),\n",
       " array([38.20580196, 28.99206553,  8.40056667]),\n",
       " array([0.50910302, 0.50324042, 0.50000001]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(entropies, axis=0), np.std(entropies, axis=0), np.std(\n",
    "    entropies, axis=0\n",
    ") / np.mean(entropies, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "52f31aea-8cd9-4ea8-8d6a-b99228185d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(ds.whole_img_vec_set.X[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e3ab50b-7008-447f-b045-363b6e6e5ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 339, 1604,   45])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ds.whole_img_vec_set.y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd37861f-0139-4607-8803-06498c10d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 1, Test set entropy : 45.036827195467424 Train samples : 354 Accuracy : 0.7705382436260623\n",
      "Fold : 2, Test set entropy : 45.040604343720496 Train samples : 707 Accuracy : 0.7648725212464589\n",
      "Fold : 3, Test set entropy : 47.038715769593956 Train samples : 1060 Accuracy : 0.7620396600566572\n",
      "Fold : 4, Test set entropy : 52.03305004721435 Train samples : 1413 Accuracy : 0.7592067988668555\n"
     ]
    }
   ],
   "source": [
    "## with straigh vectorization of whole images, the knn accuracy goes down as more data is added. Therefore that vecotrization is shit. Maybe we try tabular data?\n",
    "\n",
    "kfold_idx_gen = StratifiedKFold(n_splits=5).split(\n",
    "    np.zeros(len(ds.whole_img_vec_set)), np.sum(ds.whole_img_vec_set.y, axis=1)\n",
    ")\n",
    "train_idx = np.array([], int)\n",
    "running_train = []\n",
    "for k, (_, test_idx) in enumerate(kfold_idx_gen):\n",
    "    running_train.append(test_idx)\n",
    "running_test = running_train.pop()\n",
    "for k, old_test_idx in enumerate(running_train):\n",
    "    train_idx = np.append(train_idx,old_test_idx)\n",
    "    knn.fit(ds.whole_img_vec_set.X[train_idx], ds.whole_img_vec_set.y[train_idx])\n",
    "    y_predicted = knn.predict_proba(ds.whole_img_vec_set.X[running_test])\n",
    "    y_true = ds.whole_img_vec_set.y[running_test]\n",
    "    H = prediction_entropy(y_true, y_predicted)\n",
    "    entropies.append(H)\n",
    "    print(f\"Fold : {k+1}, \" f\"Test set entropy : {np.mean(H)}\", f\"Train samples : {len(train_idx)}\", f\"Accuracy : {accuracy_score(y_true,knn.predict(ds.whole_img_vec_set.X[test_idx]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcce42d3-8d73-4b1d-9fa0-b81cfbd43923",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hs, ss \u001b[38;5;241m=\u001b[39m \u001b[43madd_stratified_folds_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhole_img_vec_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhole_img_vec_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mknn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/better-than-vibes/btv.py:66\u001b[0m, in \u001b[0;36madd_stratified_folds_test\u001b[0;34m(X, y, clf, n_splits)\u001b[0m\n\u001b[1;32m     64\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     65\u001b[0m running_train, entropies, train_idx, scores \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, (_, test_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kfold_idx_gen):\n\u001b[1;32m     67\u001b[0m     running_train\u001b[38;5;241m.\u001b[39mappend(test_idx)\n\u001b[1;32m     68\u001b[0m running_test \u001b[38;5;241m=\u001b[39m running_train\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/.virtualenvs/study/lib/python3.10/site-packages/sklearn/model_selection/_split.py:352\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m         (\n\u001b[1;32m    347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[1;32m    350\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/.virtualenvs/study/lib/python3.10/site-packages/sklearn/model_selection/_split.py:85\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     83\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m     84\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_test_masks(X, y, groups):\n\u001b[1;32m     86\u001b[0m     train_index \u001b[38;5;241m=\u001b[39m indices[np\u001b[38;5;241m.\u001b[39mlogical_not(test_index)]\n\u001b[1;32m     87\u001b[0m     test_index \u001b[38;5;241m=\u001b[39m indices[test_index]\n",
      "File \u001b[0;32m~/.virtualenvs/study/lib/python3.10/site-packages/sklearn/model_selection/_split.py:733\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 733\u001b[0m     test_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m test_folds \u001b[38;5;241m==\u001b[39m i\n",
      "File \u001b[0;32m~/.virtualenvs/study/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    674\u001b[0m allowed_target_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_of_target_y \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_target_types:\n\u001b[0;32m--> 676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported target types are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    678\u001b[0m             allowed_target_types, type_of_target_y\n\u001b[1;32m    679\u001b[0m         )\n\u001b[1;32m    680\u001b[0m     )\n\u001b[1;32m    682\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y)\n\u001b[1;32m    684\u001b[0m _, y_idx, y_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead."
     ]
    }
   ],
   "source": [
    "hs, ss = add_stratified_folds_test(ds.whole_img_vec_set.X,ds.whole_img_vec_set.y,knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334d5b81-c0c8-4baa-98a0-2b92b35d3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning to find mean and STD of pixel values...: 26Image file [00:00, 252.12Image file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2318113 1.4534428 1.6422439] [1.05350365 1.09799279 1.13203385]\n",
      "[1.6700602 1.7795748 1.770059 ] [0.68418367 0.63546383 0.50279166]\n",
      "[1.4173375 1.244182  1.5806733] [0.79497451 0.7747967  0.84323634]\n",
      "[1.5912611 1.7805898 1.9391171] [0.73882404 0.77192257 0.77372492]\n",
      "[1.7403489 1.8966837 2.3457093] [0.73287141 0.74516506 0.79400794]\n",
      "[1.740151  1.7675205 1.8884574] [0.70134139 0.70224798 0.70604278]\n",
      "[2.1762805 2.540266  2.7033827] [0.72503405 0.75686584 0.76954361]\n",
      "[1.9214222 1.9743167 2.2817085] [0.67340267 0.67351271 0.70092843]\n",
      "[2.0022497 2.217053  2.7195246] [0.66842555 0.6875553  0.72438791]\n",
      "[1.739645  1.8590896 2.1123595] [0.62644317 0.63594593 0.65467818]\n",
      "[1.5354294 1.7255011 1.8372145] [0.59529719 0.61385796 0.62266401]\n",
      "[1.4350318 1.8585143 2.4046593] [0.574747   0.61498836 0.65835409]\n",
      "[1.6912606 1.8210794 2.0635042] [0.59064344 0.59939689 0.6163135 ]\n",
      "[0.93127114 0.98015565 1.2459649 ] [0.4903784  0.49654667 0.52964188]\n",
      "[1.938669  1.9799638 2.1793985] [0.59506623 0.59807507 0.61174235]\n",
      "[1.2231563 1.0980662 1.1724504] [0.51179676 0.49622258 0.50400211]\n",
      "[1.848824  1.9352827 2.1908846] [0.56886053 0.57625914 0.59469863]\n",
      "[2.0794156 2.2230291 2.5101185] [0.57551546 0.58532111 0.60333111]\n",
      "[1.7120212 1.6919191 1.7566024] [0.53888815 0.53653775 0.54071051]\n",
      "[1.415456  1.4753405 2.2026122] [0.50792209 0.51367561 0.57040004]\n",
      "[1.5445777 1.7491723 2.117049 ] [0.51506895 0.53179388 0.55629938]\n",
      "[1.4605552 1.568694  1.7444886] [0.50137371 0.5100318  0.52321585]\n",
      "[1.3751173 1.231069  1.4091166] [0.48870357 0.47424233 0.49076235]\n",
      "[1.6865422 1.6040411 2.1144555] [0.51045904 0.50439654 0.54103656]\n",
      "[1.1470057 1.0497869 1.1269044] [0.45580743 0.44553375 0.45175246]\n",
      "[1.6248556 1.6557782 1.7379081] [0.49663473 0.49933527 0.50539359]\n",
      "[1.1205215 1.4499464 2.2276483] [0.44509579 0.47614482 0.53197772]\n",
      "[1.8105812 1.8954669 2.1775818] [0.50151691 0.50658778 0.52316129]\n",
      "[1.5820659 1.6369601 1.845288 ] [0.47838322 0.4824491  0.49706934]\n",
      "[1.587753  1.7259104 1.9645149] [0.47563488 0.48586041 0.50188711]\n",
      "[1.3715255 1.4693712 1.7441684] [0.45428828 0.4621094  0.48254007]\n",
      "[1.7790056 2.06681   2.5299811] [0.48264278 0.50132029 0.52739964]\n",
      "[1.8365784 1.9824116 2.285174 ] [0.48207838 0.49108236 0.5085829 ]\n",
      "[1.6001091 1.6804193 1.839653 ] [0.4617848  0.46736954 0.4778506 ]\n",
      "[1.2151229 1.1624235 1.2961445] [0.42755985 0.42244819 0.43419868]\n",
      "[1.3497113 1.635075  2.2656913] [0.4372829  0.45938104 0.49889346]\n",
      "[1.8155918 1.873027  2.1154485] [0.46826885 0.47149044 0.48537035]\n",
      "[0.8207751 0.8508402 1.3397243] [0.37762078 0.3810828  0.42876006]\n",
      "[1.8806419 1.7925129 1.9037986] [0.46733688 0.46165167 0.4679446 ]\n",
      "[1.6067103 1.688439  2.1150496] [0.4443214  0.45018972 0.47678811]\n",
      "[1.6896617 1.4779959 1.6958649] [0.44794035 0.43267541 0.44749879]\n",
      "[1.5935962 1.6836675 1.8664087] [0.43853474 0.44511053 0.45664751]\n",
      "[1.920124  2.0987659 2.4174478] [0.45746043 0.46781487 0.48473171]\n",
      "[1.7396744 1.7798331 2.0848548] [0.44308357 0.44542017 0.4634383 ]\n",
      "[2.091354  2.1902268 2.372203 ] [0.46215124 0.46757377 0.47681816]\n",
      "[1.5634817 1.8885956 2.3731718] [0.42620519 0.44726211 0.47397107]\n",
      "[1.8493321 1.698295  1.9749628] [0.44337839 0.43340094 0.44983659]\n",
      "[1.3860368 1.258568  1.3886969] [0.40932015 0.39955048 0.4093474 ]\n",
      "[1.9869345 2.0254529 2.3099844] [0.44714877 0.44947623 0.46454248]\n",
      "[1.715394  1.5886514 1.7206503] [0.42784675 0.41945823 0.42776141]\n",
      "[1.8615117 2.0111465 2.3633235] [0.43511264 0.443899   0.46231615]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning to find mean and STD of pixel values...: 100Image file [00:00, 250.21Image file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8751795 1.7735482 1.7269161] [0.43367506 0.4273725  0.42403932]\n",
      "[1.4566951 1.6566691 1.9193112] [0.40467302 0.41834188 0.43438302]\n",
      "[1.8546876 1.9543949 2.3294976] [0.42893145 0.43444985 0.45398856]\n",
      "[1.5723968 1.6128262 1.7960677] [0.40896585 0.41151065 0.42256277]\n",
      "[1.5420915 1.3962276 1.9071897] [0.40550192 0.39531099 0.42778056]\n",
      "[2.2680368 2.442622  2.7910063] [0.44528873 0.45384171 0.46898711]\n",
      "[1.8816338 2.0341432 2.2751508] [0.42216687 0.43047704 0.44265156]\n",
      "[1.924601  1.9740739 2.1826773] [0.42321913 0.42581655 0.436624  ]\n",
      "[1.5619034 1.4240656 1.8282518] [0.39959714 0.39021926 0.41571062]\n",
      "[1.3886795 1.5259602 1.805834 ] [0.38663934 0.39617853 0.41307259]\n",
      "[1.8811905 2.118001  2.361266 ] [0.41611443 0.42866037 0.44039388]\n",
      "[1.3134642 1.4211087 1.6223089] [0.37780037 0.38522189 0.39824578]\n",
      "[2.1004784 2.1697516 2.3953824] [0.42459645 0.42800608 0.43868478]\n",
      "[1.5756496 1.5196352 1.7781814] [0.39253063 0.38884989 0.40455772]\n",
      "[1.7674989 1.8518579 2.1095457] [0.40316756 0.40800657 0.42147689]\n",
      "[1.6441286 1.7429458 1.9318451] [0.39419066 0.40000197 0.41037998]\n",
      "[1.6126232 1.6232862 1.7805572] [0.39094719 0.39151248 0.40065296]\n",
      "[1.8406895 1.8518589 2.173042 ] [0.40285258 0.40346267 0.42000999]\n",
      "[1.6279833 1.6842879 1.9725811] [0.38892608 0.39229056 0.40809161]\n",
      "[1.6772969 1.6623542 2.165309 ] [0.39070288 0.38976998 0.41654861]\n",
      "[1.7945622 1.9332902 2.0581868] [0.39603858 0.40358701 0.40967376]\n",
      "[1.5801498 1.398977  1.5536053] [0.38206759 0.37029098 0.38020506]\n",
      "[1.5703784 1.6506876 1.7995591] [0.3803731  0.38535818 0.39374538]\n",
      "[2.0014417 2.1810164 2.4262173] [0.4031149  0.41190402 0.42304458]\n",
      "[1.6443399 1.7304314 2.0225155] [0.38197659 0.3868244  0.40228738]\n",
      "[1.8517667 1.7718029 1.9952465] [0.39266162 0.3882401  0.39989119]\n",
      "[1.474883  1.5411934 1.743413 ] [0.36931885 0.37353231 0.38523175]\n",
      "[1.889757  2.0104733 2.507788 ] [0.39230203 0.39843855 0.42117076]\n",
      "[1.5187824 1.5070362 1.7650857] [0.36973864 0.36891589 0.38367721]\n",
      "[1.8779727 2.100557  2.6422002] [0.38923795 0.40040612 0.42410757]\n",
      "[1.4210596 1.4655261 1.7218325] [0.36135288 0.36401874 0.37886529]\n",
      "[0.86292917 0.45729533 0.60345846] [0.31772951 0.26979172 0.28948137]\n",
      "[1.6553268 1.7575635 1.6314863] [0.37409388 0.38004074 0.37291315]\n",
      "[1.5667552 1.5921955 1.7278813] [0.3673075  0.3687334  0.37653185]\n",
      "[1.8965979 2.0169134 2.2792473] [0.38443433 0.39043385 0.40258862]\n",
      "[1.6867834 1.7879099 1.9769378] [0.37193748 0.37738637 0.38695999]\n",
      "[1.6815819 1.8128537 2.090573 ] [0.37073717 0.37778818 0.39153891]\n",
      "[2.147298  2.1237047 2.2838187] [0.39324766 0.39208442 0.39920396]\n",
      "[1.7239386 1.6614884 1.6262522] [0.37072607 0.36728929 0.36519716]\n",
      "[1.7438488 1.7157301 1.9866742] [0.37105167 0.3695678  0.38352603]\n",
      "[1.7475351 1.7097552 1.875547 ] [0.37023322 0.36820987 0.37676908]\n",
      "[1.3572302 1.456589  1.84594  ] [0.34636109 0.3526425  0.37431902]\n",
      "[2.0416408 2.3184507 2.7723594] [0.38321703 0.3956315  0.41367458]\n",
      "[1.7645594 1.9519602 2.0689666] [0.36803829 0.37741179 0.38278925]\n",
      "[2.112629  2.2818942 2.6429124] [0.38431821 0.3917747  0.40650628]\n",
      "[2.1791337 2.2922373 2.7879257] [0.3861763  0.39106624 0.41073214]\n",
      "[1.3478459 1.5445658 1.8099924] [0.34103351 0.35297053 0.3671909 ]\n",
      "[1.6737621 1.802904  2.0376394] [0.35986004 0.36656039 0.37792014]\n",
      "[1.9292635 2.2771823 2.6142812] [0.37187757 0.38769054 0.40131702]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imc = ds.imc[:100]\n",
    "color = len(imc[0].shape) > 2\n",
    "channels = imc[0].shape[2] if color else 1\n",
    "    \n",
    "px_mean, px_std = np.zeros(channels), np.zeros(channels)\n",
    "running_sum,running_sumsq  = 0,0 \n",
    "for n, im in tqdm(\n",
    "    enumerate(imc), desc=\"Scanning to find mean and STD of pixel values...\", unit=\"Image file\"\n",
    "):\n",
    "\n",
    "    last_px_mean = px_mean.copy()\n",
    "    last_px_std = px_std.copy()\n",
    "    px_mean += (im.mean(0).mean(0) - px_mean)/(n + 1)\n",
    "    px_std = (im.std(0).std(0) - last_px_std)/(n + 1)\n",
    "\n",
    "    print(im.std(0).std(0), np.power(px_std,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3caedb7b-8895-424c-bcf3-e0a1a8801a24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43my_predicted\u001b[49m, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m      3\u001b[0m     y_predicted \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(y_predicted, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     classes \u001b[38;5;241m=\u001b[39m y_predicted\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_predicted' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0956a7d-4cfd-4259-b00f-bc7b4b2f75f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
